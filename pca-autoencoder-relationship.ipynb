{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn import datasets\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_dim = 5\n",
    "cov = sklearn.datasets.make_spd_matrix(n_dim, random_state=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mu = np.random.normal(0, 0.1, n_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = np.random.multivariate_normal(mu, cov, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.51050356,  0.0979012 , -1.3533993 ,  1.69652164,  0.12368509],\n",
       "       [ 0.0979012 ,  0.38203196,  0.0838268 , -0.32166331,  0.1182334 ],\n",
       "       [-1.3533993 ,  0.0838268 ,  1.70201001, -1.75043085,  0.03068494],\n",
       "       [ 1.69652164, -0.32166331, -1.75043085,  3.04370879, -0.02026801],\n",
       "       [ 0.12368509,  0.1182334 ,  0.03068494, -0.02026801,  0.52746562]])"
      ]
     },
     "execution_count": 406,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.48408819,  0.0869628 , -1.37851651,  1.75915641,  0.11585292],\n",
       "       [ 0.0869628 ,  0.37814901,  0.05120384, -0.31708924,  0.11503449],\n",
       "       [-1.37851651,  0.05120384,  1.78657098, -1.87388981,  0.05312682],\n",
       "       [ 1.75915641, -0.31708924, -1.87388981,  3.17433478, -0.04601059],\n",
       "       [ 0.11585292,  0.11503449,  0.05312682, -0.04601059,  0.6029139 ]])"
      ]
     },
     "execution_count": 407,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.cov(X.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler().fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_scaled = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-2.1316282072803006e-17"
      ]
     },
     "execution_count": 411,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(X_scaled[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pca = decomposition.PCA()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PCA(copy=True, iterated_power='auto', n_components=None, random_state=None,\n",
       "  svd_solver='auto', tol=0.0, whiten=False)"
      ]
     },
     "execution_count": 414,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca.fit(X_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.64588048, 1.31158911, 0.78598256, 0.18109259, 0.0854753 ])"
      ]
     },
     "execution_count": 415,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca.explained_variance_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pca_output = pca.fit_transform(X_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.65, -0.  ,  0.  ,  0.  ,  0.  ],\n",
       "       [-0.  ,  1.31,  0.  , -0.  ,  0.  ],\n",
       "       [ 0.  ,  0.  ,  0.79,  0.  ,  0.  ],\n",
       "       [ 0.  , -0.  ,  0.  ,  0.18, -0.  ],\n",
       "       [ 0.  ,  0.  ,  0.  , -0.  ,  0.09]])"
      ]
     },
     "execution_count": 417,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(np.cov(pca_output.T), 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try a linear autoencoder to mimic PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input, Dense\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "from keras import regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_24 (InputLayer)        (None, 5)                 0         \n",
      "_________________________________________________________________\n",
      "dense_113 (Dense)            (None, 2)                 10        \n",
      "_________________________________________________________________\n",
      "dense_114 (Dense)            (None, 5)                 10        \n",
      "=================================================================\n",
      "Total params: 20\n",
      "Trainable params: 20\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "nb_epoch = 100\n",
    "batch_size = 16\n",
    "input_dim = X_scaled.shape[1] #num of predictor variables, \n",
    "encoding_dim = 2\n",
    "learning_rate = 1e-3\n",
    "\n",
    "input_layer = Input(shape=(input_dim, ))\n",
    "encoder = Dense(encoding_dim, activation=\"linear\", use_bias = False)(input_layer)\n",
    "decoder = Dense(input_dim, activation=\"linear\", use_bias = False)(encoder)\n",
    "autoencoder = Model(inputs=input_layer, outputs=decoder)\n",
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "500/500 [==============================] - 2s 3ms/step - loss: 1.3535 - acc: 0.1940\n",
      "Epoch 2/100\n",
      "500/500 [==============================] - 0s 79us/step - loss: 1.1982 - acc: 0.2320\n",
      "Epoch 3/100\n",
      "500/500 [==============================] - 0s 74us/step - loss: 1.0717 - acc: 0.2560\n",
      "Epoch 4/100\n",
      "500/500 [==============================] - 0s 78us/step - loss: 0.9652 - acc: 0.2780\n",
      "Epoch 5/100\n",
      "500/500 [==============================] - 0s 77us/step - loss: 0.8740 - acc: 0.3140\n",
      "Epoch 6/100\n",
      "500/500 [==============================] - 0s 75us/step - loss: 0.7949 - acc: 0.3380\n",
      "Epoch 7/100\n",
      "500/500 [==============================] - 0s 73us/step - loss: 0.7262 - acc: 0.3520\n",
      "Epoch 8/100\n",
      "500/500 [==============================] - 0s 73us/step - loss: 0.6674 - acc: 0.3640\n",
      "Epoch 9/100\n",
      "500/500 [==============================] - 0s 78us/step - loss: 0.6171 - acc: 0.3860\n",
      "Epoch 10/100\n",
      "500/500 [==============================] - 0s 74us/step - loss: 0.5740 - acc: 0.4100\n",
      "Epoch 11/100\n",
      "500/500 [==============================] - 0s 76us/step - loss: 0.5368 - acc: 0.4300\n",
      "Epoch 12/100\n",
      "500/500 [==============================] - 0s 84us/step - loss: 0.5046 - acc: 0.4500\n",
      "Epoch 13/100\n",
      "500/500 [==============================] - 0s 73us/step - loss: 0.4759 - acc: 0.4740\n",
      "Epoch 14/100\n",
      "500/500 [==============================] - 0s 74us/step - loss: 0.4501 - acc: 0.5040\n",
      "Epoch 15/100\n",
      "500/500 [==============================] - 0s 75us/step - loss: 0.4274 - acc: 0.5120\n",
      "Epoch 16/100\n",
      "500/500 [==============================] - 0s 75us/step - loss: 0.4056 - acc: 0.5340\n",
      "Epoch 17/100\n",
      "500/500 [==============================] - 0s 72us/step - loss: 0.3859 - acc: 0.5540\n",
      "Epoch 18/100\n",
      "500/500 [==============================] - 0s 77us/step - loss: 0.3674 - acc: 0.5600\n",
      "Epoch 19/100\n",
      "500/500 [==============================] - 0s 73us/step - loss: 0.3503 - acc: 0.5720\n",
      "Epoch 20/100\n",
      "500/500 [==============================] - 0s 72us/step - loss: 0.3346 - acc: 0.5760\n",
      "Epoch 21/100\n",
      "500/500 [==============================] - 0s 75us/step - loss: 0.3194 - acc: 0.5700\n",
      "Epoch 22/100\n",
      "500/500 [==============================] - 0s 79us/step - loss: 0.3057 - acc: 0.5780\n",
      "Epoch 23/100\n",
      "500/500 [==============================] - 0s 72us/step - loss: 0.2939 - acc: 0.5920\n",
      "Epoch 24/100\n",
      "500/500 [==============================] - 0s 72us/step - loss: 0.2832 - acc: 0.5980\n",
      "Epoch 25/100\n",
      "500/500 [==============================] - 0s 75us/step - loss: 0.2744 - acc: 0.5960\n",
      "Epoch 26/100\n",
      "500/500 [==============================] - 0s 84us/step - loss: 0.2667 - acc: 0.6000\n",
      "Epoch 27/100\n",
      "500/500 [==============================] - 0s 89us/step - loss: 0.2604 - acc: 0.6060\n",
      "Epoch 28/100\n",
      "500/500 [==============================] - 0s 85us/step - loss: 0.2551 - acc: 0.6120\n",
      "Epoch 29/100\n",
      "500/500 [==============================] - 0s 84us/step - loss: 0.2507 - acc: 0.6140\n",
      "Epoch 30/100\n",
      "500/500 [==============================] - 0s 84us/step - loss: 0.2471 - acc: 0.6180\n",
      "Epoch 31/100\n",
      "500/500 [==============================] - 0s 81us/step - loss: 0.2442 - acc: 0.6220\n",
      "Epoch 32/100\n",
      "500/500 [==============================] - 0s 88us/step - loss: 0.2418 - acc: 0.6260\n",
      "Epoch 33/100\n",
      "500/500 [==============================] - 0s 80us/step - loss: 0.2398 - acc: 0.6180\n",
      "Epoch 34/100\n",
      "500/500 [==============================] - 0s 81us/step - loss: 0.2383 - acc: 0.6200\n",
      "Epoch 35/100\n",
      "500/500 [==============================] - 0s 81us/step - loss: 0.2368 - acc: 0.6200\n",
      "Epoch 36/100\n",
      "500/500 [==============================] - 0s 86us/step - loss: 0.2358 - acc: 0.6220\n",
      "Epoch 37/100\n",
      "500/500 [==============================] - 0s 89us/step - loss: 0.2347 - acc: 0.6240\n",
      "Epoch 38/100\n",
      "500/500 [==============================] - 0s 80us/step - loss: 0.2338 - acc: 0.6220\n",
      "Epoch 39/100\n",
      "500/500 [==============================] - 0s 77us/step - loss: 0.2330 - acc: 0.6180\n",
      "Epoch 40/100\n",
      "500/500 [==============================] - 0s 78us/step - loss: 0.2321 - acc: 0.6160\n",
      "Epoch 41/100\n",
      "500/500 [==============================] - 0s 76us/step - loss: 0.2313 - acc: 0.6160\n",
      "Epoch 42/100\n",
      "500/500 [==============================] - 0s 74us/step - loss: 0.2307 - acc: 0.6180\n",
      "Epoch 43/100\n",
      "500/500 [==============================] - 0s 74us/step - loss: 0.2300 - acc: 0.6200\n",
      "Epoch 44/100\n",
      "500/500 [==============================] - 0s 78us/step - loss: 0.2293 - acc: 0.6200\n",
      "Epoch 45/100\n",
      "500/500 [==============================] - 0s 78us/step - loss: 0.2287 - acc: 0.6220\n",
      "Epoch 46/100\n",
      "500/500 [==============================] - 0s 82us/step - loss: 0.2281 - acc: 0.6200\n",
      "Epoch 47/100\n",
      "500/500 [==============================] - 0s 80us/step - loss: 0.2276 - acc: 0.6180\n",
      "Epoch 48/100\n",
      "500/500 [==============================] - 0s 80us/step - loss: 0.2268 - acc: 0.6160\n",
      "Epoch 49/100\n",
      "500/500 [==============================] - 0s 80us/step - loss: 0.2262 - acc: 0.6180\n",
      "Epoch 50/100\n",
      "500/500 [==============================] - 0s 79us/step - loss: 0.2256 - acc: 0.6180\n",
      "Epoch 51/100\n",
      "500/500 [==============================] - 0s 78us/step - loss: 0.2251 - acc: 0.6180\n",
      "Epoch 52/100\n",
      "500/500 [==============================] - 0s 75us/step - loss: 0.2245 - acc: 0.6200\n",
      "Epoch 53/100\n",
      "500/500 [==============================] - 0s 85us/step - loss: 0.2240 - acc: 0.6180\n",
      "Epoch 54/100\n",
      "500/500 [==============================] - 0s 96us/step - loss: 0.2236 - acc: 0.6200\n",
      "Epoch 55/100\n",
      "500/500 [==============================] - 0s 89us/step - loss: 0.2230 - acc: 0.6220\n",
      "Epoch 56/100\n",
      "500/500 [==============================] - 0s 85us/step - loss: 0.2227 - acc: 0.6200\n",
      "Epoch 57/100\n",
      "500/500 [==============================] - 0s 88us/step - loss: 0.2222 - acc: 0.6200\n",
      "Epoch 58/100\n",
      "500/500 [==============================] - 0s 84us/step - loss: 0.2218 - acc: 0.6200\n",
      "Epoch 59/100\n",
      "500/500 [==============================] - 0s 86us/step - loss: 0.2213 - acc: 0.6220\n",
      "Epoch 60/100\n",
      "500/500 [==============================] - 0s 84us/step - loss: 0.2208 - acc: 0.6240\n",
      "Epoch 61/100\n",
      "500/500 [==============================] - 0s 84us/step - loss: 0.2204 - acc: 0.6280\n",
      "Epoch 62/100\n",
      "500/500 [==============================] - 0s 83us/step - loss: 0.2199 - acc: 0.6280\n",
      "Epoch 63/100\n",
      "500/500 [==============================] - 0s 88us/step - loss: 0.2197 - acc: 0.6260\n",
      "Epoch 64/100\n",
      "500/500 [==============================] - 0s 83us/step - loss: 0.2193 - acc: 0.6260\n",
      "Epoch 65/100\n",
      "500/500 [==============================] - 0s 88us/step - loss: 0.2188 - acc: 0.6280\n",
      "Epoch 66/100\n",
      "500/500 [==============================] - 0s 84us/step - loss: 0.2185 - acc: 0.6280\n",
      "Epoch 67/100\n",
      "500/500 [==============================] - 0s 84us/step - loss: 0.2180 - acc: 0.6300\n",
      "Epoch 68/100\n",
      "500/500 [==============================] - 0s 82us/step - loss: 0.2178 - acc: 0.6300\n",
      "Epoch 69/100\n",
      "500/500 [==============================] - 0s 84us/step - loss: 0.2174 - acc: 0.6340\n",
      "Epoch 70/100\n",
      "500/500 [==============================] - 0s 84us/step - loss: 0.2170 - acc: 0.6320\n",
      "Epoch 71/100\n",
      "500/500 [==============================] - 0s 83us/step - loss: 0.2167 - acc: 0.6340\n",
      "Epoch 72/100\n",
      "500/500 [==============================] - 0s 85us/step - loss: 0.2164 - acc: 0.6300\n",
      "Epoch 73/100\n",
      "500/500 [==============================] - 0s 75us/step - loss: 0.2161 - acc: 0.6300\n",
      "Epoch 74/100\n",
      "500/500 [==============================] - 0s 76us/step - loss: 0.2158 - acc: 0.6300\n",
      "Epoch 75/100\n",
      "500/500 [==============================] - 0s 70us/step - loss: 0.2155 - acc: 0.6320\n",
      "Epoch 76/100\n",
      "500/500 [==============================] - 0s 76us/step - loss: 0.2153 - acc: 0.6340\n",
      "Epoch 77/100\n",
      "500/500 [==============================] - 0s 80us/step - loss: 0.2150 - acc: 0.6340\n",
      "Epoch 78/100\n",
      "500/500 [==============================] - 0s 76us/step - loss: 0.2148 - acc: 0.6340\n",
      "Epoch 79/100\n",
      "500/500 [==============================] - 0s 74us/step - loss: 0.2146 - acc: 0.6340\n",
      "Epoch 80/100\n",
      "500/500 [==============================] - 0s 72us/step - loss: 0.2143 - acc: 0.6320\n",
      "Epoch 81/100\n",
      "500/500 [==============================] - 0s 74us/step - loss: 0.2141 - acc: 0.6320\n",
      "Epoch 82/100\n",
      "500/500 [==============================] - 0s 75us/step - loss: 0.2140 - acc: 0.6340\n",
      "Epoch 83/100\n",
      "500/500 [==============================] - 0s 75us/step - loss: 0.2138 - acc: 0.6320\n",
      "Epoch 84/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 0s 75us/step - loss: 0.2137 - acc: 0.6320\n",
      "Epoch 85/100\n",
      "500/500 [==============================] - 0s 74us/step - loss: 0.2134 - acc: 0.6320\n",
      "Epoch 86/100\n",
      "500/500 [==============================] - 0s 72us/step - loss: 0.2133 - acc: 0.6320\n",
      "Epoch 87/100\n",
      "500/500 [==============================] - 0s 73us/step - loss: 0.2131 - acc: 0.6320\n",
      "Epoch 88/100\n",
      "500/500 [==============================] - 0s 74us/step - loss: 0.2130 - acc: 0.6320\n",
      "Epoch 89/100\n",
      "500/500 [==============================] - 0s 73us/step - loss: 0.2129 - acc: 0.6300\n",
      "Epoch 90/100\n",
      "500/500 [==============================] - 0s 72us/step - loss: 0.2126 - acc: 0.6320\n",
      "Epoch 91/100\n",
      "500/500 [==============================] - 0s 74us/step - loss: 0.2125 - acc: 0.6280\n",
      "Epoch 92/100\n",
      "500/500 [==============================] - 0s 73us/step - loss: 0.2124 - acc: 0.6320\n",
      "Epoch 93/100\n",
      "500/500 [==============================] - 0s 72us/step - loss: 0.2122 - acc: 0.6320\n",
      "Epoch 94/100\n",
      "500/500 [==============================] - 0s 73us/step - loss: 0.2122 - acc: 0.6320\n",
      "Epoch 95/100\n",
      "500/500 [==============================] - 0s 73us/step - loss: 0.2120 - acc: 0.6320\n",
      "Epoch 96/100\n",
      "500/500 [==============================] - 0s 73us/step - loss: 0.2121 - acc: 0.6300\n",
      "Epoch 97/100\n",
      "500/500 [==============================] - 0s 74us/step - loss: 0.2120 - acc: 0.6320\n",
      "Epoch 98/100\n",
      "500/500 [==============================] - 0s 77us/step - loss: 0.2118 - acc: 0.6320\n",
      "Epoch 99/100\n",
      "500/500 [==============================] - 0s 75us/step - loss: 0.2116 - acc: 0.6300\n",
      "Epoch 100/100\n",
      "500/500 [==============================] - 0s 75us/step - loss: 0.2116 - acc: 0.6280\n"
     ]
    }
   ],
   "source": [
    "autoencoder.compile(metrics=['accuracy'],\n",
    "                    loss='mean_squared_error',\n",
    "                    optimizer='adam')\n",
    "\n",
    "history = autoencoder.fit(X_scaled, X_scaled,\n",
    "                    epochs=nb_epoch,\n",
    "                    batch_size=batch_size,\n",
    "                    shuffle=True,\n",
    "                    verbose=1).history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[-0.6 ,  0.44],\n",
       "        [-0.88, -0.15],\n",
       "        [-0.01, -0.41],\n",
       "        [ 0.13,  0.4 ],\n",
       "        [-0.72, -0.06]]], dtype=float32)"
      ]
     },
     "execution_count": 421,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(autoencoder.layers[1].get_weights(), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[-0.25, -0.57,  0.07,  0.05, -0.47],\n",
       "        [ 0.73, -0.27, -0.77,  0.79, -0.12]]], dtype=float32)"
      ]
     },
     "execution_count": 422,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(autoencoder.layers[2].get_weights(), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.57542321, -0.0823935 , -0.57578763,  0.57494914,  0.00123692],\n",
       "       [-0.21854994, -0.71220901,  0.01300828,  0.13110063, -0.6539401 ],\n",
       "       [ 0.10062666,  0.62637442, -0.13465058, -0.14418562, -0.74740156],\n",
       "       [-0.17979115, -0.15745114, -0.75680923, -0.6007439 ,  0.09607725],\n",
       "       [ 0.7607059 , -0.26236209,  0.27822364, -0.52015708, -0.0672375 ]])"
      ]
     },
     "execution_count": 423,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.52811774, 0.26179319, 0.15688212, 0.03614608, 0.01706087])"
      ]
     },
     "execution_count": 424,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "intermediate_layer = Model(inputs=autoencoder.inputs, outputs=autoencoder.layers[1].output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "intermediate_output = intermediate_layer.predict(X_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 2)"
      ]
     },
     "execution_count": 427,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intermediate_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "aa = np.array(intermediate_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cc = np.cov(aa.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.15649179, -0.20868458],\n",
       "       [-0.20868458,  1.42645623]])"
      ]
     },
     "execution_count": 430,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "aa_scaler = StandardScaler().fit(aa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "aa_scaled = aa_scaler.fit_transform(aa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pca0 = decomposition.PCA()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PCA(copy=True, iterated_power='auto', n_components=None, random_state=None,\n",
       "  svd_solver='auto', tol=0.0, whiten=False)"
      ]
     },
     "execution_count": 434,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca0.fit(aa_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.5594918 , 0.44050822], dtype=float32)"
      ]
     },
     "execution_count": 435,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca0.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1430,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Layer, InputSpec\n",
    "from keras import activations, initializers, constraints, Sequential\n",
    "from keras import backend as K\n",
    "from keras.constraints import UnitNorm\n",
    "\n",
    "# Reference: https://stackoverflow.com/questions/53751024/tying-autoencoder-weights-in-a-dense-keras-layer\n",
    "class DenseTied(Layer):\n",
    "    def __init__(self, units,\n",
    "                 activation=None,\n",
    "                 use_bias=True,\n",
    "                 kernel_initializer='glorot_uniform',\n",
    "                 bias_initializer='zeros',\n",
    "                 kernel_regularizer=None,\n",
    "                 bias_regularizer=None,\n",
    "                 activity_regularizer=None,\n",
    "                 kernel_constraint=None,\n",
    "                 bias_constraint=None,\n",
    "                 tied_to=None,\n",
    "                 **kwargs):\n",
    "        self.tied_to = tied_to\n",
    "        if 'input_shape' not in kwargs and 'input_dim' in kwargs:\n",
    "            kwargs['input_shape'] = (kwargs.pop('input_dim'),)\n",
    "        super().__init__(**kwargs)\n",
    "        self.units = units\n",
    "        self.activation = activations.get(activation)\n",
    "        self.use_bias = use_bias\n",
    "        self.kernel_initializer = initializers.get(kernel_initializer)\n",
    "        self.bias_initializer = initializers.get(bias_initializer)\n",
    "        self.kernel_regularizer = regularizers.get(kernel_regularizer)\n",
    "        self.bias_regularizer = regularizers.get(bias_regularizer)\n",
    "        self.activity_regularizer = regularizers.get(activity_regularizer)\n",
    "        self.kernel_constraint = constraints.get(kernel_constraint)\n",
    "        self.bias_constraint = constraints.get(bias_constraint)\n",
    "        self.input_spec = InputSpec(min_ndim=2)\n",
    "        self.supports_masking = True\n",
    "        \n",
    "#     def __init__(self, units,\n",
    "#              activation=None,\n",
    "#              use_bias=True,\n",
    "#              kernel_initializer='glorot_uniform',\n",
    "#              bias_initializer='zeros',\n",
    "#              kernel_regularizer=None,\n",
    "#              bias_regularizer=None,\n",
    "#              activity_regularizer=None,\n",
    "#              kernel_constraint=None,\n",
    "#              bias_constraint=None,\n",
    "#              tied_to=None,\n",
    "#              **kwargs):\n",
    "#         if 'input_shape' not in kwargs and 'input_dim' in kwargs:\n",
    "#             kwargs['input_shape'] = (kwargs.pop('input_dim'),)\n",
    "#         super(Dense, self).__init__(**kwargs)\n",
    "#         self.units = units\n",
    "#         self.activation = activations.get(activation)\n",
    "#         self.use_bias = use_bias\n",
    "#         self.kernel_initializer = initializers.get(kernel_initializer)\n",
    "#         self.bias_initializer = initializers.get(bias_initializer)\n",
    "#         self.kernel_regularizer = regularizers.get(kernel_regularizer)\n",
    "#         self.bias_regularizer = regularizers.get(bias_regularizer)\n",
    "#         self.activity_regularizer = regularizers.get(activity_regularizer)\n",
    "#         self.kernel_constraint = constraints.get(kernel_constraint)\n",
    "#         self.bias_constraint = constraints.get(bias_constraint)\n",
    "#         self.input_spec = InputSpec(min_ndim=2)\n",
    "#         self.supports_masking = True\n",
    "#         self.tied_to = tied_to\n",
    "\n",
    "#     def build(self, input_shape):\n",
    "#         assert len(input_shape) >= 2\n",
    "#         input_dim = input_shape[-1]\n",
    "\n",
    "#         if self.tied_to is not None:\n",
    "#             self.kernel = K.transpose(self.tied_to.kernel)\n",
    "#             self._non_trainable_weights.append(self.kernel)\n",
    "#         else:\n",
    "#             self.kernel = self.add_weight(shape=(input_dim, self.units),\n",
    "#                                           initializer=self.kernel_initializer,\n",
    "#                                           name='kernel',\n",
    "#                                           regularizer=self.kernel_regularizer,\n",
    "#                                           constraint=self.kernel_constraint)\n",
    "#         if self.use_bias:\n",
    "#             self.bias = self.add_weight(shape=(self.units,),\n",
    "#                                         initializer=self.bias_initializer,\n",
    "#                                         name='bias',\n",
    "#                                         regularizer=self.bias_regularizer,\n",
    "#                                         constraint=self.bias_constraint)\n",
    "#         else:\n",
    "#             self.bias = None\n",
    "\n",
    "#         self.built = True\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) >= 2\n",
    "        input_dim = input_shape[-1]\n",
    "\n",
    "        if self.tied_to is not None:\n",
    "            self.kernel = K.transpose(self.tied_to.kernel)\n",
    "            self._non_trainable_weights.append(self.kernel)\n",
    "        else:\n",
    "            self.kernel = self.add_weight(shape=(input_dim, self.units),\n",
    "                                          initializer=self.kernel_initializer,\n",
    "                                          name='kernel',\n",
    "                                          regularizer=self.kernel_regularizer,\n",
    "                                          constraint=self.kernel_constraint)\n",
    "        if self.use_bias:\n",
    "            self.bias = self.add_weight(shape=(self.units,),\n",
    "                                        initializer=self.bias_initializer,\n",
    "                                        name='bias',\n",
    "                                        regularizer=self.bias_regularizer,\n",
    "                                        constraint=self.bias_constraint)\n",
    "        else:\n",
    "            self.bias = None\n",
    "        self.input_spec = InputSpec(min_ndim=2, axes={-1: input_dim})\n",
    "        self.built = True\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        assert input_shape and len(input_shape) >= 2\n",
    "#         assert input_shape[-1] == self.units\n",
    "        output_shape = list(input_shape)\n",
    "        output_shape[-1] = self.units\n",
    "        return tuple(output_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        output = K.dot(inputs, self.kernel)\n",
    "        if self.use_bias:\n",
    "            output = K.bias_add(output, self.bias, data_format='channels_last')\n",
    "        if self.activation is not None:\n",
    "            output = self.activation(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1479,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_142 (Dense)            (None, 1)                 6         \n",
      "_________________________________________________________________\n",
      "dense_tied_63 (DenseTied)    (None, 5)                 5         \n",
      "=================================================================\n",
      "Total params: 11\n",
      "Trainable params: 6\n",
      "Non-trainable params: 5\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# encoded1 = Dense(4, activation=\"sigmoid\", input_shape=(4,), use_bias=True)\n",
    "# decoded1 = DenseTied(4, activation=\"sigmoid\", tied_to=encoded1, use_bias=False)\n",
    "\n",
    "# # autoencoder\n",
    "# #\n",
    "# autoencoder = Sequential()\n",
    "# # autoencoder.add(input_)\n",
    "# autoencoder.add(encoded1)\n",
    "# autoencoder.add(decoded1)\n",
    "\n",
    "# autoencoder.compile(optimizer=\"adam\", loss=\"binary_crossentropy\")\n",
    "\n",
    "# print(autoencoder.summary())\n",
    "\n",
    "# autoencoder.fit(x=np.random.rand(100, 4), y=np.random.randint(0, 1, size=(100, 4)))\n",
    "\n",
    "# print(autoencoder.layers[0].get_weights()[0])\n",
    "# print(autoencoder.layers[1].get_weights()[0])\n",
    "\n",
    "# input_ = Input(shape=(16,), dtype=np.float32)\n",
    "# encoder\n",
    "#\n",
    "\n",
    "def fro_norm(w):\n",
    "    return K.sqrt(K.sum(K.square(K.abs(w))))\n",
    "\n",
    "def cust_reg(w):\n",
    "    if(encoding_dim > 1):\n",
    "        m = K.dot(K.transpose(w), w) - K.eye(encoding_dim)\n",
    "        return fro_norm(m)\n",
    "    else:\n",
    "        m = K.sum(w ** 2) - 1.\n",
    "        return m\n",
    "    \n",
    "nb_epoch = 100\n",
    "batch_size = 16\n",
    "input_dim = X_scaled.shape[1] #num of predictor variables, \n",
    "encoding_dim = 1\n",
    "learning_rate = 1e-3\n",
    "encoder = Dense(encoding_dim, activation=\"linear\", input_shape=(input_dim,), use_bias = True, kernel_regularizer=cust_reg, kernel_constraint=UnitNorm(axis=0)) \n",
    "decoder = DenseTied(input_dim, activation=\"linear\", tied_to=encoder, use_bias = False)\n",
    "\n",
    "autoencoder = Sequential()\n",
    "autoencoder.add(encoder)\n",
    "autoencoder.add(decoder)\n",
    "\n",
    "autoencoder.compile(metrics=['accuracy'],\n",
    "                    loss='mean_squared_error',\n",
    "                    optimizer='sgd')\n",
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1484,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "500/500 [==============================] - 0s 82us/step - loss: 0.4722 - acc: 0.4360\n",
      "Epoch 2/100\n",
      "500/500 [==============================] - 0s 92us/step - loss: 0.4722 - acc: 0.4180\n",
      "Epoch 3/100\n",
      "500/500 [==============================] - 0s 82us/step - loss: 0.4722 - acc: 0.4020\n",
      "Epoch 4/100\n",
      "500/500 [==============================] - 0s 86us/step - loss: 0.4722 - acc: 0.3980\n",
      "Epoch 5/100\n",
      "500/500 [==============================] - 0s 81us/step - loss: 0.4722 - acc: 0.4540\n",
      "Epoch 6/100\n",
      "500/500 [==============================] - 0s 88us/step - loss: 0.4722 - acc: 0.4020\n",
      "Epoch 7/100\n",
      "500/500 [==============================] - 0s 80us/step - loss: 0.4722 - acc: 0.4020\n",
      "Epoch 8/100\n",
      "500/500 [==============================] - 0s 80us/step - loss: 0.4721 - acc: 0.4180\n",
      "Epoch 9/100\n",
      "500/500 [==============================] - 0s 80us/step - loss: 0.4722 - acc: 0.4600\n",
      "Epoch 10/100\n",
      "500/500 [==============================] - 0s 82us/step - loss: 0.4723 - acc: 0.3980\n",
      "Epoch 11/100\n",
      "500/500 [==============================] - 0s 82us/step - loss: 0.4721 - acc: 0.4560\n",
      "Epoch 12/100\n",
      "500/500 [==============================] - 0s 88us/step - loss: 0.4722 - acc: 0.4260\n",
      "Epoch 13/100\n",
      "500/500 [==============================] - 0s 84us/step - loss: 0.4722 - acc: 0.4600\n",
      "Epoch 14/100\n",
      "500/500 [==============================] - 0s 82us/step - loss: 0.4722 - acc: 0.4480\n",
      "Epoch 15/100\n",
      "500/500 [==============================] - 0s 82us/step - loss: 0.4723 - acc: 0.4000\n",
      "Epoch 16/100\n",
      "500/500 [==============================] - 0s 80us/step - loss: 0.4722 - acc: 0.4620\n",
      "Epoch 17/100\n",
      "500/500 [==============================] - 0s 82us/step - loss: 0.4722 - acc: 0.4000\n",
      "Epoch 18/100\n",
      "500/500 [==============================] - 0s 79us/step - loss: 0.4722 - acc: 0.4120\n",
      "Epoch 19/100\n",
      "500/500 [==============================] - 0s 81us/step - loss: 0.4722 - acc: 0.4220\n",
      "Epoch 20/100\n",
      "500/500 [==============================] - 0s 81us/step - loss: 0.4721 - acc: 0.4040\n",
      "Epoch 21/100\n",
      "500/500 [==============================] - 0s 85us/step - loss: 0.4723 - acc: 0.4720\n",
      "Epoch 22/100\n",
      "500/500 [==============================] - 0s 88us/step - loss: 0.4722 - acc: 0.4220\n",
      "Epoch 23/100\n",
      "500/500 [==============================] - 0s 94us/step - loss: 0.4722 - acc: 0.4240\n",
      "Epoch 24/100\n",
      "500/500 [==============================] - 0s 93us/step - loss: 0.4722 - acc: 0.4320\n",
      "Epoch 25/100\n",
      "500/500 [==============================] - 0s 93us/step - loss: 0.4722 - acc: 0.4040\n",
      "Epoch 26/100\n",
      "500/500 [==============================] - 0s 89us/step - loss: 0.4721 - acc: 0.4160\n",
      "Epoch 27/100\n",
      "500/500 [==============================] - 0s 87us/step - loss: 0.4721 - acc: 0.4260\n",
      "Epoch 28/100\n",
      "500/500 [==============================] - 0s 98us/step - loss: 0.4722 - acc: 0.4520\n",
      "Epoch 29/100\n",
      "500/500 [==============================] - 0s 96us/step - loss: 0.4721 - acc: 0.3980\n",
      "Epoch 30/100\n",
      "500/500 [==============================] - 0s 89us/step - loss: 0.4722 - acc: 0.4320\n",
      "Epoch 31/100\n",
      "500/500 [==============================] - 0s 88us/step - loss: 0.4722 - acc: 0.4040\n",
      "Epoch 32/100\n",
      "500/500 [==============================] - 0s 87us/step - loss: 0.4722 - acc: 0.4060\n",
      "Epoch 33/100\n",
      "500/500 [==============================] - 0s 89us/step - loss: 0.4721 - acc: 0.4680\n",
      "Epoch 34/100\n",
      "500/500 [==============================] - 0s 90us/step - loss: 0.4722 - acc: 0.4680\n",
      "Epoch 35/100\n",
      "500/500 [==============================] - 0s 91us/step - loss: 0.4722 - acc: 0.4280\n",
      "Epoch 36/100\n",
      "500/500 [==============================] - 0s 85us/step - loss: 0.4722 - acc: 0.4060\n",
      "Epoch 37/100\n",
      "500/500 [==============================] - 0s 100us/step - loss: 0.4723 - acc: 0.4680\n",
      "Epoch 38/100\n",
      "500/500 [==============================] - 0s 96us/step - loss: 0.4721 - acc: 0.4060\n",
      "Epoch 39/100\n",
      "500/500 [==============================] - 0s 95us/step - loss: 0.4722 - acc: 0.4420\n",
      "Epoch 40/100\n",
      "500/500 [==============================] - 0s 89us/step - loss: 0.4721 - acc: 0.4800\n",
      "Epoch 41/100\n",
      "500/500 [==============================] - 0s 86us/step - loss: 0.4723 - acc: 0.4660\n",
      "Epoch 42/100\n",
      "500/500 [==============================] - 0s 94us/step - loss: 0.4722 - acc: 0.4520\n",
      "Epoch 43/100\n",
      "500/500 [==============================] - 0s 92us/step - loss: 0.4721 - acc: 0.4700\n",
      "Epoch 44/100\n",
      "500/500 [==============================] - 0s 89us/step - loss: 0.4723 - acc: 0.4600\n",
      "Epoch 45/100\n",
      "500/500 [==============================] - 0s 90us/step - loss: 0.4722 - acc: 0.4800\n",
      "Epoch 46/100\n",
      "500/500 [==============================] - 0s 90us/step - loss: 0.4722 - acc: 0.4800\n",
      "Epoch 47/100\n",
      "500/500 [==============================] - 0s 91us/step - loss: 0.4721 - acc: 0.4620\n",
      "Epoch 48/100\n",
      "500/500 [==============================] - 0s 93us/step - loss: 0.4722 - acc: 0.4620\n",
      "Epoch 49/100\n",
      "500/500 [==============================] - 0s 92us/step - loss: 0.4721 - acc: 0.4020\n",
      "Epoch 50/100\n",
      "500/500 [==============================] - 0s 90us/step - loss: 0.4722 - acc: 0.4400\n",
      "Epoch 51/100\n",
      "500/500 [==============================] - 0s 89us/step - loss: 0.4721 - acc: 0.4040\n",
      "Epoch 52/100\n",
      "500/500 [==============================] - 0s 93us/step - loss: 0.4722 - acc: 0.3980\n",
      "Epoch 53/100\n",
      "500/500 [==============================] - 0s 95us/step - loss: 0.4722 - acc: 0.4700\n",
      "Epoch 54/100\n",
      "500/500 [==============================] - 0s 105us/step - loss: 0.4722 - acc: 0.4640\n",
      "Epoch 55/100\n",
      "500/500 [==============================] - 0s 98us/step - loss: 0.4722 - acc: 0.4480\n",
      "Epoch 56/100\n",
      "500/500 [==============================] - 0s 104us/step - loss: 0.4722 - acc: 0.4440\n",
      "Epoch 57/100\n",
      "500/500 [==============================] - 0s 93us/step - loss: 0.4722 - acc: 0.4620\n",
      "Epoch 58/100\n",
      "500/500 [==============================] - 0s 90us/step - loss: 0.4721 - acc: 0.4040\n",
      "Epoch 59/100\n",
      "500/500 [==============================] - 0s 93us/step - loss: 0.4722 - acc: 0.4120\n",
      "Epoch 60/100\n",
      "500/500 [==============================] - 0s 82us/step - loss: 0.4723 - acc: 0.4040\n",
      "Epoch 61/100\n",
      "500/500 [==============================] - 0s 82us/step - loss: 0.4722 - acc: 0.3940\n",
      "Epoch 62/100\n",
      "500/500 [==============================] - 0s 81us/step - loss: 0.4722 - acc: 0.4040\n",
      "Epoch 63/100\n",
      "500/500 [==============================] - 0s 85us/step - loss: 0.4722 - acc: 0.4020\n",
      "Epoch 64/100\n",
      "500/500 [==============================] - 0s 87us/step - loss: 0.4722 - acc: 0.4040\n",
      "Epoch 65/100\n",
      "500/500 [==============================] - 0s 94us/step - loss: 0.4722 - acc: 0.4040\n",
      "Epoch 66/100\n",
      "500/500 [==============================] - 0s 95us/step - loss: 0.4723 - acc: 0.4040\n",
      "Epoch 67/100\n",
      "500/500 [==============================] - 0s 96us/step - loss: 0.4722 - acc: 0.4040\n",
      "Epoch 68/100\n",
      "500/500 [==============================] - 0s 93us/step - loss: 0.4723 - acc: 0.4060\n",
      "Epoch 69/100\n",
      "500/500 [==============================] - 0s 95us/step - loss: 0.4722 - acc: 0.4120\n",
      "Epoch 70/100\n",
      "500/500 [==============================] - 0s 92us/step - loss: 0.4722 - acc: 0.4680\n",
      "Epoch 71/100\n",
      "500/500 [==============================] - 0s 91us/step - loss: 0.4722 - acc: 0.4360\n",
      "Epoch 72/100\n",
      "500/500 [==============================] - 0s 88us/step - loss: 0.4722 - acc: 0.4060\n",
      "Epoch 73/100\n",
      "500/500 [==============================] - 0s 89us/step - loss: 0.4722 - acc: 0.4180\n",
      "Epoch 74/100\n",
      "500/500 [==============================] - 0s 90us/step - loss: 0.4722 - acc: 0.4340\n",
      "Epoch 75/100\n",
      "500/500 [==============================] - 0s 90us/step - loss: 0.4722 - acc: 0.4800\n",
      "Epoch 76/100\n",
      "500/500 [==============================] - 0s 90us/step - loss: 0.4722 - acc: 0.4800\n",
      "Epoch 77/100\n",
      "500/500 [==============================] - 0s 88us/step - loss: 0.4723 - acc: 0.4780\n",
      "Epoch 78/100\n",
      "500/500 [==============================] - 0s 92us/step - loss: 0.4722 - acc: 0.4780\n",
      "Epoch 79/100\n",
      "500/500 [==============================] - 0s 88us/step - loss: 0.4721 - acc: 0.4200\n",
      "Epoch 80/100\n",
      "500/500 [==============================] - 0s 93us/step - loss: 0.4721 - acc: 0.4040\n",
      "Epoch 81/100\n",
      "500/500 [==============================] - 0s 93us/step - loss: 0.4722 - acc: 0.3980\n",
      "Epoch 82/100\n",
      "500/500 [==============================] - 0s 94us/step - loss: 0.4722 - acc: 0.4020\n",
      "Epoch 83/100\n",
      "500/500 [==============================] - 0s 92us/step - loss: 0.4722 - acc: 0.4820\n",
      "Epoch 84/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 0s 92us/step - loss: 0.4722 - acc: 0.4800\n",
      "Epoch 85/100\n",
      "500/500 [==============================] - 0s 92us/step - loss: 0.4722 - acc: 0.4480\n",
      "Epoch 86/100\n",
      "500/500 [==============================] - 0s 93us/step - loss: 0.4722 - acc: 0.4140\n",
      "Epoch 87/100\n",
      "500/500 [==============================] - 0s 94us/step - loss: 0.4721 - acc: 0.4240\n",
      "Epoch 88/100\n",
      "500/500 [==============================] - 0s 94us/step - loss: 0.4722 - acc: 0.4680\n",
      "Epoch 89/100\n",
      "500/500 [==============================] - 0s 93us/step - loss: 0.4722 - acc: 0.4180\n",
      "Epoch 90/100\n",
      "500/500 [==============================] - 0s 91us/step - loss: 0.4723 - acc: 0.4680\n",
      "Epoch 91/100\n",
      "500/500 [==============================] - 0s 89us/step - loss: 0.4721 - acc: 0.4240\n",
      "Epoch 92/100\n",
      "500/500 [==============================] - 0s 87us/step - loss: 0.4722 - acc: 0.4140\n",
      "Epoch 93/100\n",
      "500/500 [==============================] - 0s 80us/step - loss: 0.4723 - acc: 0.4000\n",
      "Epoch 94/100\n",
      "500/500 [==============================] - 0s 87us/step - loss: 0.4722 - acc: 0.4620\n",
      "Epoch 95/100\n",
      "500/500 [==============================] - 0s 86us/step - loss: 0.4722 - acc: 0.4720\n",
      "Epoch 96/100\n",
      "500/500 [==============================] - 0s 79us/step - loss: 0.4722 - acc: 0.4800\n",
      "Epoch 97/100\n",
      "500/500 [==============================] - 0s 89us/step - loss: 0.4722 - acc: 0.4720\n",
      "Epoch 98/100\n",
      "500/500 [==============================] - 0s 79us/step - loss: 0.4722 - acc: 0.4520\n",
      "Epoch 99/100\n",
      "500/500 [==============================] - 0s 85us/step - loss: 0.4722 - acc: 0.4500\n",
      "Epoch 100/100\n",
      "500/500 [==============================] - 0s 85us/step - loss: 0.4723 - acc: 0.4480\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x14bcbd0f0>"
      ]
     },
     "execution_count": 1484,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autoencoder.fit(X_scaled, X_scaled,\n",
    "                    epochs=nb_epoch,\n",
    "                    batch_size=batch_size,\n",
    "                    shuffle=True,\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1485,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.573  0.084  0.579 -0.574 -0.007]]\n",
      "[[-0.573  0.084  0.579 -0.574 -0.007]]\n"
     ]
    }
   ],
   "source": [
    "print(np.round(np.transpose(autoencoder.layers[0].get_weights()[0]), 3))\n",
    "print(np.round(autoencoder.layers[1].get_weights()[0], 3))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1475,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.9999999], dtype=float32)"
      ]
     },
     "execution_count": 1475,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# scipy.linalg.norm(np.transpose(autoencoder.layers[0].get_weights()[0]), 2)\n",
    "np.sum(np.transpose(autoencoder.layers[0].get_weights()[0]) ** 2, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1464,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.575, -0.082, -0.576,  0.575,  0.001],\n",
       "       [-0.219, -0.712,  0.013,  0.131, -0.654],\n",
       "       [ 0.101,  0.626, -0.135, -0.144, -0.747],\n",
       "       [-0.18 , -0.157, -0.757, -0.601,  0.096],\n",
       "       [ 0.761, -0.262,  0.278, -0.52 , -0.067]])"
      ]
     },
     "execution_count": 1464,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(pca.components_, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  0.,  0.,  0., -0.],\n",
       "       [ 0.,  1., -0.,  0., -0.],\n",
       "       [ 0., -0.,  1., -0.,  0.],\n",
       "       [ 0.,  0., -0.,  1., -0.],\n",
       "       [-0., -0.,  0., -0.,  1.]])"
      ]
     },
     "execution_count": 445,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(np.dot(pca.components_, np.transpose(pca.components_)), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1., -0.,  0.,  0.,  0.],\n",
       "       [-0.,  1.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  1., -0., -0.],\n",
       "       [ 0.,  0., -0.,  1., -0.],\n",
       "       [ 0.,  0., -0., -0.,  1.]], dtype=float32)"
      ]
     },
     "execution_count": 446,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(np.dot(autoencoder.layers[0].get_weights()[0], np.transpose(autoencoder.layers[0].get_weights()[0])), 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "-------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n",
      "Epoch 1/100\n",
      "100/100 [==============================] - 1s 13ms/step - loss: 13.0799 - acc: 0.4200\n",
      "Epoch 2/100\n",
      "100/100 [==============================] - 0s 78us/step - loss: 12.7055 - acc: 0.4200\n",
      "Epoch 3/100\n",
      "100/100 [==============================] - 0s 83us/step - loss: 12.3557 - acc: 0.4200\n",
      "Epoch 4/100\n",
      "100/100 [==============================] - 0s 70us/step - loss: 12.0283 - acc: 0.4200\n",
      "Epoch 5/100\n",
      "100/100 [==============================] - 0s 82us/step - loss: 11.7216 - acc: 0.4200\n",
      "Epoch 6/100\n",
      "100/100 [==============================] - 0s 72us/step - loss: 11.4338 - acc: 0.4200\n",
      "Epoch 7/100\n",
      "100/100 [==============================] - 0s 74us/step - loss: 11.1634 - acc: 0.4200\n",
      "Epoch 8/100\n",
      "100/100 [==============================] - 0s 70us/step - loss: 10.9091 - acc: 0.4200\n",
      "Epoch 9/100\n",
      "100/100 [==============================] - 0s 84us/step - loss: 10.6695 - acc: 0.4200\n",
      "Epoch 10/100\n",
      "100/100 [==============================] - 0s 69us/step - loss: 10.4436 - acc: 0.4200\n",
      "Epoch 11/100\n",
      "100/100 [==============================] - 0s 75us/step - loss: 10.2302 - acc: 0.4200\n",
      "Epoch 12/100\n",
      "100/100 [==============================] - 0s 77us/step - loss: 10.0283 - acc: 0.4200\n",
      "Epoch 13/100\n",
      "100/100 [==============================] - 0s 83us/step - loss: 9.8369 - acc: 0.4200\n",
      "Epoch 14/100\n",
      "100/100 [==============================] - 0s 73us/step - loss: 9.6549 - acc: 0.4200\n",
      "Epoch 15/100\n",
      "100/100 [==============================] - 0s 81us/step - loss: 9.4815 - acc: 0.4200\n",
      "Epoch 16/100\n",
      "100/100 [==============================] - 0s 75us/step - loss: 9.3162 - acc: 0.4200\n",
      "Epoch 17/100\n",
      "100/100 [==============================] - 0s 89us/step - loss: 9.2698 - acc: 0.4200\n",
      "Epoch 18/100\n",
      "100/100 [==============================] - 0s 75us/step - loss: 9.2697 - acc: 0.4200\n",
      "Epoch 19/100\n",
      "100/100 [==============================] - 0s 82us/step - loss: 9.2695 - acc: 0.4200\n",
      "Epoch 20/100\n",
      "100/100 [==============================] - 0s 66us/step - loss: 9.2694 - acc: 0.4200\n",
      "Epoch 21/100\n",
      "100/100 [==============================] - 0s 68us/step - loss: 9.2693 - acc: 0.4200\n",
      "Epoch 22/100\n",
      "100/100 [==============================] - 0s 75us/step - loss: 9.2692 - acc: 0.4200\n",
      "Epoch 23/100\n",
      "100/100 [==============================] - 0s 85us/step - loss: 9.2690 - acc: 0.4200\n",
      "Epoch 24/100\n",
      "100/100 [==============================] - 0s 77us/step - loss: 9.2689 - acc: 0.4200\n",
      "Epoch 25/100\n",
      "100/100 [==============================] - 0s 87us/step - loss: 9.2687 - acc: 0.4200\n",
      "Epoch 26/100\n",
      "100/100 [==============================] - 0s 73us/step - loss: 9.2686 - acc: 0.4200\n",
      "Epoch 27/100\n",
      "100/100 [==============================] - 0s 90us/step - loss: 9.2685 - acc: 0.4200\n",
      "Epoch 28/100\n",
      "100/100 [==============================] - 0s 80us/step - loss: 9.2683 - acc: 0.4200\n",
      "Epoch 29/100\n",
      "100/100 [==============================] - 0s 91us/step - loss: 9.2681 - acc: 0.4200\n",
      "Epoch 30/100\n",
      "100/100 [==============================] - 0s 72us/step - loss: 9.2680 - acc: 0.4200\n",
      "Epoch 31/100\n",
      "100/100 [==============================] - 0s 83us/step - loss: 9.2678 - acc: 0.4200\n",
      "Epoch 32/100\n",
      "100/100 [==============================] - 0s 75us/step - loss: 9.2677 - acc: 0.4200\n",
      "Epoch 33/100\n",
      "100/100 [==============================] - 0s 71us/step - loss: 9.2675 - acc: 0.4200\n",
      "Epoch 34/100\n",
      "100/100 [==============================] - 0s 75us/step - loss: 9.2674 - acc: 0.4200\n",
      "Epoch 35/100\n",
      "100/100 [==============================] - 0s 77us/step - loss: 9.2672 - acc: 0.4200\n",
      "Epoch 36/100\n",
      "100/100 [==============================] - 0s 84us/step - loss: 9.2671 - acc: 0.4200\n",
      "Epoch 37/100\n",
      "100/100 [==============================] - 0s 79us/step - loss: 9.2669 - acc: 0.4200\n",
      "Epoch 38/100\n",
      "100/100 [==============================] - 0s 86us/step - loss: 9.2667 - acc: 0.4200\n",
      "Epoch 39/100\n",
      "100/100 [==============================] - 0s 75us/step - loss: 9.2666 - acc: 0.4200\n",
      "Epoch 40/100\n",
      "100/100 [==============================] - 0s 88us/step - loss: 9.2664 - acc: 0.4200\n",
      "Epoch 41/100\n",
      "100/100 [==============================] - 0s 77us/step - loss: 9.2663 - acc: 0.4200\n",
      "Epoch 42/100\n",
      "100/100 [==============================] - 0s 86us/step - loss: 9.2661 - acc: 0.4200\n",
      "Epoch 43/100\n",
      "100/100 [==============================] - 0s 80us/step - loss: 9.2660 - acc: 0.4200\n",
      "Epoch 44/100\n",
      "100/100 [==============================] - 0s 87us/step - loss: 9.2658 - acc: 0.4200\n",
      "Epoch 45/100\n",
      "100/100 [==============================] - 0s 72us/step - loss: 9.2656 - acc: 0.4200\n",
      "Epoch 46/100\n",
      "100/100 [==============================] - 0s 77us/step - loss: 9.2655 - acc: 0.4200\n",
      "Epoch 47/100\n",
      "100/100 [==============================] - 0s 68us/step - loss: 9.2653 - acc: 0.4200\n",
      "Epoch 48/100\n",
      "100/100 [==============================] - 0s 81us/step - loss: 9.2652 - acc: 0.4200\n",
      "Epoch 49/100\n",
      "100/100 [==============================] - 0s 78us/step - loss: 9.2650 - acc: 0.4200\n",
      "Epoch 50/100\n",
      "100/100 [==============================] - 0s 85us/step - loss: 9.2649 - acc: 0.4200\n",
      "Epoch 51/100\n",
      "100/100 [==============================] - 0s 79us/step - loss: 9.2647 - acc: 0.4200\n",
      "Epoch 52/100\n",
      "100/100 [==============================] - 0s 76us/step - loss: 9.2646 - acc: 0.4200\n",
      "Epoch 53/100\n",
      "100/100 [==============================] - 0s 84us/step - loss: 9.2644 - acc: 0.4200\n",
      "Epoch 54/100\n",
      "100/100 [==============================] - 0s 68us/step - loss: 9.2643 - acc: 0.4200\n",
      "Epoch 55/100\n",
      "100/100 [==============================] - 0s 85us/step - loss: 9.2641 - acc: 0.4200\n",
      "Epoch 56/100\n",
      "100/100 [==============================] - 0s 74us/step - loss: 9.2640 - acc: 0.4200\n",
      "Epoch 57/100\n",
      "100/100 [==============================] - 0s 85us/step - loss: 9.2639 - acc: 0.4200\n",
      "Epoch 58/100\n",
      "100/100 [==============================] - 0s 70us/step - loss: 9.2637 - acc: 0.4200\n",
      "Epoch 59/100\n",
      "100/100 [==============================] - 0s 78us/step - loss: 9.2636 - acc: 0.4200\n",
      "Epoch 60/100\n",
      "100/100 [==============================] - 0s 66us/step - loss: 9.2635 - acc: 0.4200\n",
      "Epoch 61/100\n",
      "100/100 [==============================] - 0s 83us/step - loss: 9.2634 - acc: 0.4200\n",
      "Epoch 62/100\n",
      "100/100 [==============================] - 0s 71us/step - loss: 9.2633 - acc: 0.4200\n",
      "Epoch 63/100\n",
      "100/100 [==============================] - 0s 81us/step - loss: 9.2632 - acc: 0.4200\n",
      "Epoch 64/100\n",
      "100/100 [==============================] - 0s 71us/step - loss: 9.2632 - acc: 0.4200\n",
      "Epoch 65/100\n",
      "100/100 [==============================] - 0s 77us/step - loss: 9.2631 - acc: 0.4200\n",
      "Epoch 66/100\n",
      "100/100 [==============================] - 0s 68us/step - loss: 9.2631 - acc: 0.4200\n",
      "Epoch 67/100\n",
      "100/100 [==============================] - 0s 73us/step - loss: 9.2632 - acc: 0.4200\n",
      "Epoch 68/100\n",
      "100/100 [==============================] - 0s 69us/step - loss: 9.2633 - acc: 0.4200\n",
      "Epoch 69/100\n",
      "100/100 [==============================] - 0s 64us/step - loss: 9.2635 - acc: 0.4200\n",
      "Epoch 70/100\n",
      "100/100 [==============================] - 0s 74us/step - loss: 9.2637 - acc: 0.4200\n",
      "Epoch 71/100\n",
      "100/100 [==============================] - 0s 76us/step - loss: 9.2640 - acc: 0.4200\n",
      "Epoch 72/100\n",
      "100/100 [==============================] - 0s 80us/step - loss: 9.2643 - acc: 0.4200\n",
      "Epoch 73/100\n",
      "100/100 [==============================] - 0s 91us/step - loss: 9.2646 - acc: 0.4200\n",
      "Epoch 74/100\n",
      "100/100 [==============================] - 0s 82us/step - loss: 9.2649 - acc: 0.4200\n",
      "Epoch 75/100\n",
      "100/100 [==============================] - 0s 80us/step - loss: 9.2652 - acc: 0.4200\n",
      "Epoch 76/100\n",
      "100/100 [==============================] - 0s 82us/step - loss: 9.2656 - acc: 0.4200\n",
      "Epoch 77/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 10.4795 - acc: 0.34 - 0s 95us/step - loss: 9.2659 - acc: 0.4200\n",
      "Epoch 78/100\n",
      "100/100 [==============================] - 0s 75us/step - loss: 9.2663 - acc: 0.4200\n",
      "Epoch 79/100\n",
      "100/100 [==============================] - 0s 71us/step - loss: 9.2666 - acc: 0.4200\n",
      "Epoch 80/100\n",
      "100/100 [==============================] - 0s 81us/step - loss: 9.2670 - acc: 0.4200\n",
      "Epoch 81/100\n",
      "100/100 [==============================] - 0s 84us/step - loss: 9.2674 - acc: 0.4200\n",
      "Epoch 82/100\n",
      "100/100 [==============================] - 0s 72us/step - loss: 9.2677 - acc: 0.4200\n",
      "Epoch 83/100\n",
      "100/100 [==============================] - 0s 90us/step - loss: 9.2681 - acc: 0.4200\n",
      "Epoch 84/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 0s 86us/step - loss: 9.2684 - acc: 0.4200\n",
      "Epoch 85/100\n",
      "100/100 [==============================] - 0s 67us/step - loss: 9.2687 - acc: 0.4200\n",
      "Epoch 86/100\n",
      "100/100 [==============================] - 0s 81us/step - loss: 9.2690 - acc: 0.4200\n",
      "Epoch 87/100\n",
      "100/100 [==============================] - 0s 72us/step - loss: 9.2693 - acc: 0.4200\n",
      "Epoch 88/100\n",
      "100/100 [==============================] - 0s 70us/step - loss: 9.2696 - acc: 0.4200\n",
      "Epoch 89/100\n",
      "100/100 [==============================] - 0s 70us/step - loss: 9.2698 - acc: 0.4200\n",
      "Epoch 90/100\n",
      "100/100 [==============================] - 0s 66us/step - loss: 9.2700 - acc: 0.4200\n",
      "Epoch 91/100\n",
      "100/100 [==============================] - 0s 76us/step - loss: 9.2701 - acc: 0.4200\n",
      "Epoch 92/100\n",
      "100/100 [==============================] - 0s 75us/step - loss: 9.2701 - acc: 0.4200\n",
      "Epoch 93/100\n",
      "100/100 [==============================] - 0s 71us/step - loss: 9.2701 - acc: 0.4200\n",
      "Epoch 94/100\n",
      "100/100 [==============================] - 0s 71us/step - loss: 9.2701 - acc: 0.4200\n",
      "Epoch 95/100\n",
      "100/100 [==============================] - 0s 75us/step - loss: 9.2700 - acc: 0.4200\n",
      "Epoch 96/100\n",
      "100/100 [==============================] - 0s 70us/step - loss: 9.2699 - acc: 0.4200\n",
      "Epoch 97/100\n",
      "100/100 [==============================] - 0s 71us/step - loss: 9.2698 - acc: 0.4200\n",
      "Epoch 98/100\n",
      "100/100 [==============================] - 0s 69us/step - loss: 9.2697 - acc: 0.4200\n",
      "Epoch 99/100\n",
      "100/100 [==============================] - 0s 69us/step - loss: 9.2696 - acc: 0.4200\n",
      "Epoch 100/100\n",
      "100/100 [==============================] - 0s 84us/step - loss: 9.2694 - acc: 0.4200\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1436a9748>"
      ]
     },
     "execution_count": 349,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras import backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "\n",
    "a_dim = 16\n",
    "from keras import backend as K\n",
    "def fro_norm(w):\n",
    "    return K.sqrt(K.sum(K.square(K.abs(w))))\n",
    "\n",
    "def cust_reg(w):\n",
    "    print(w.shape[1])\n",
    "    m = K.dot(K.transpose(w), w) - np.eye(a_dim)\n",
    "    return fro_norm(m)\n",
    "\n",
    "X = np.random.randn(100, 100)\n",
    "y = np.random.randint(2, size=(100, 1))\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "\n",
    "# apply regularization here. applies regularization to the \n",
    "# output (activation) of the layer\n",
    "model.add(Dense(a_dim, input_shape=(100,), \n",
    "                kernel_regularizer=cust_reg))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss=\"binary_crossentropy\",\n",
    "              optimizer='sgd',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(X, y, epochs=100, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_104 (Dense)            (None, 16)                1616      \n",
      "_________________________________________________________________\n",
      "dense_105 (Dense)            (None, 1)                 17        \n",
      "_________________________________________________________________\n",
      "activation_15 (Activation)   (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 1,633\n",
      "Trainable params: 1,633\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "intermediate_layer = Model(inputs=model.inputs, outputs=model.layers[0].output)\n",
    "intermediate_output = intermediate_layer.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 16)"
      ]
     },
     "execution_count": 352,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intermediate_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.01,  0.  ,  0.  , -0.  ,  0.  , -0.  , -0.  , -0.  , -0.  ,\n",
       "        -0.  ,  0.  , -0.  , -0.  , -0.  ,  0.  ,  0.  ],\n",
       "       [ 0.  ,  1.01,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,\n",
       "         0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  , -0.  ],\n",
       "       [ 0.  ,  0.  ,  1.  ,  0.  ,  0.  , -0.  ,  0.  ,  0.  , -0.  ,\n",
       "         0.  , -0.  , -0.  ,  0.  , -0.  ,  0.  ,  0.  ],\n",
       "       [-0.  ,  0.  ,  0.  ,  1.01,  0.  , -0.  ,  0.  , -0.  ,  0.  ,\n",
       "        -0.  , -0.  ,  0.  , -0.  , -0.  , -0.  ,  0.  ],\n",
       "       [ 0.  ,  0.  ,  0.  ,  0.  ,  1.  ,  0.  , -0.  , -0.  , -0.  ,\n",
       "        -0.  ,  0.  , -0.  , -0.  ,  0.  , -0.  ,  0.  ],\n",
       "       [-0.  ,  0.  , -0.  , -0.  ,  0.  ,  1.01,  0.  ,  0.  , -0.  ,\n",
       "         0.  , -0.  ,  0.  ,  0.  , -0.  , -0.  , -0.  ],\n",
       "       [-0.  ,  0.  ,  0.  ,  0.  , -0.  ,  0.  ,  1.  , -0.  , -0.  ,\n",
       "        -0.  , -0.  ,  0.  , -0.  , -0.  , -0.  ,  0.  ],\n",
       "       [-0.  ,  0.  ,  0.  , -0.  , -0.  ,  0.  , -0.  ,  1.01, -0.  ,\n",
       "         0.  ,  0.  , -0.  , -0.  ,  0.  , -0.  ,  0.  ],\n",
       "       [-0.  ,  0.  , -0.  ,  0.  , -0.  , -0.  , -0.  , -0.  ,  1.  ,\n",
       "         0.  , -0.  ,  0.  , -0.  ,  0.  ,  0.  ,  0.  ],\n",
       "       [-0.  ,  0.  ,  0.  , -0.  , -0.  ,  0.  , -0.  ,  0.  ,  0.  ,\n",
       "         1.01, -0.  ,  0.  ,  0.  ,  0.  , -0.  ,  0.  ],\n",
       "       [ 0.  ,  0.  , -0.  , -0.  ,  0.  , -0.  , -0.  ,  0.  , -0.  ,\n",
       "        -0.  ,  1.  , -0.  , -0.  , -0.  ,  0.  ,  0.  ],\n",
       "       [-0.  ,  0.  , -0.  ,  0.  , -0.  ,  0.  ,  0.  , -0.  ,  0.  ,\n",
       "         0.  , -0.  ,  1.01,  0.  , -0.  ,  0.  ,  0.  ],\n",
       "       [-0.  ,  0.  ,  0.  , -0.  , -0.  ,  0.  , -0.  , -0.  , -0.  ,\n",
       "         0.  , -0.  ,  0.  ,  1.  , -0.  ,  0.  ,  0.  ],\n",
       "       [-0.  ,  0.  , -0.  , -0.  ,  0.  , -0.  , -0.  ,  0.  ,  0.  ,\n",
       "         0.  , -0.  , -0.  , -0.  ,  1.01,  0.  ,  0.  ],\n",
       "       [ 0.  ,  0.  ,  0.  , -0.  , -0.  , -0.  , -0.  , -0.  ,  0.  ,\n",
       "        -0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  1.  , -0.  ],\n",
       "       [ 0.  , -0.  ,  0.  ,  0.  ,  0.  , -0.  ,  0.  ,  0.  ,  0.  ,\n",
       "         0.  ,  0.  ,  0.  ,  0.  ,  0.  , -0.  ,  1.01]], dtype=float32)"
      ]
     },
     "execution_count": 353,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(np.dot(np.transpose(model.layers[0].get_weights()[0]), model.layers[0].get_weights()[0]), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 16)"
      ]
     },
     "execution_count": 354,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[0].get_weights()[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solve PCA by reconstruction loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scipy\n",
    "from scipy.optimize import minimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36.33585501444451"
      ]
     },
     "execution_count": 449,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scipy.linalg.norm(X_scaled, ord=2, axis=None, keepdims=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 969,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.00200401, 1.00200401, 1.00200401, 1.00200401, 1.00200401])"
      ]
     },
     "execution_count": 969,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.diag(np.cov(X_scaled.T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 959,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.64588048, 1.31158911, 0.78598256, 0.0854753 , 0.18109259])"
      ]
     },
     "execution_count": 959,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.eig(np.cov(X_scaled.T))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 960,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Vp = np.linalg.eig(np.cov(X_scaled.T))[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 963,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Z = np.dot(X_scaled, Vp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 967,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.646,  0.   , -0.   , -0.   , -0.   ],\n",
       "       [ 0.   ,  1.312,  0.   , -0.   , -0.   ],\n",
       "       [-0.   ,  0.   ,  0.786,  0.   ,  0.   ],\n",
       "       [-0.   , -0.   ,  0.   ,  0.085, -0.   ],\n",
       "       [-0.   , -0.   ,  0.   , -0.   ,  0.181]])"
      ]
     },
     "execution_count": 967,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(np.cov(Z.T), 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 962,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 912.47769054,  165.98763746, -241.61287659,  -16.78774581,\n",
       "        -481.97426312],\n",
       "       [ 165.98763746,  342.23942029, -109.53662261,   32.68936889,\n",
       "         112.62438902],\n",
       "       [-241.61287659, -109.53662261,  465.6547657 ,  206.77787717,\n",
       "          -4.85070314],\n",
       "       [ -16.78774581,   32.68936889,  206.77787717,  178.62616209,\n",
       "         -27.96399176],\n",
       "       [-481.97426312,  112.62438902,   -4.85070314,  -27.96399176,\n",
       "         601.00196138]])"
      ]
     },
     "execution_count": 962,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(Vp, np.dot(np.dot(X_scaled.T, X_scaled), Vp.T)) /"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1357,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruction_error(x):\n",
    "    V = x.reshape(-1, 5)\n",
    "    loss = 0.1 * scipy.linalg.norm((X_scaled - np.dot(X_scaled, np.dot(np.transpose(V), V))), 2) / V.shape[0] + orthogonality_constraint(V) + norm_constraint(V) + max_variance(V)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1358,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.57542321, -0.0823935 , -0.57578763,  0.57494914,  0.00123692],\n",
       "       [-0.21854994, -0.71220901,  0.01300828,  0.13110063, -0.6539401 ],\n",
       "       [ 0.10062666,  0.62637442, -0.13465058, -0.14418562, -0.74740156],\n",
       "       [-0.17979115, -0.15745114, -0.75680923, -0.6007439 ,  0.09607725],\n",
       "       [ 0.7607059 , -0.26236209,  0.27822364, -0.52015708, -0.0672375 ]])"
      ]
     },
     "execution_count": 1358,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1359,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.64588048 1.31158911 0.78598256 0.0854753  0.18109259]\n",
      "[2.64588048 1.31158911 0.78598256 0.18109259 0.0854753 ]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.13522325525487686"
      ]
     },
     "execution_count": 1359,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reconstruction_error(pca.components_.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1360,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def orthogonality_constraint(V):\n",
    "    return scipy.linalg.norm(np.dot(V, np.transpose(V)) - np.eye(V.shape[0]), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1361,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def norm_constraint(V):\n",
    "    return scipy.linalg.norm(np.sum(V ** 2, axis = 1) - np.ones(V.shape[0]), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1362,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def max_variance(V):\n",
    "    eigenvalues = np.linalg.eig(np.cov(X_scaled.T))[0][0:V.shape[0]]\n",
    "    print(eigenvalues)\n",
    "    Z_scores = np.dot(X_scaled, V.T)\n",
    "    if(V.shape[0] > 1):\n",
    "        Z_cov = np.diag(np.cov(Z_scores.T))\n",
    "    else:\n",
    "        Z_cov = np.cov(Z_scores.T)\n",
    "    print(Z_cov)\n",
    "    return scipy.linalg.norm(Z_cov - eigenvalues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1363,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.981230178556928e-16"
      ]
     },
     "execution_count": 1363,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orthogonality_constraint(pca.components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1364,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pca_reduced = decomposition.PCA(n_components=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1365,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PCA(copy=True, iterated_power='auto', n_components=2, random_state=None,\n",
       "  svd_solver='auto', tol=0.0, whiten=False)"
      ]
     },
     "execution_count": 1365,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca_reduced.fit(X_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1366,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.64588048, 1.31158911])"
      ]
     },
     "execution_count": 1366,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca_reduced.explained_variance_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1367,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.64588048 1.31158911]\n",
      "[2.64588048 1.31158911]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9902086842977708"
      ]
     },
     "execution_count": 1367,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reconstruction_error(pca_reduced.components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1368,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 5)"
      ]
     },
     "execution_count": 1368,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca_reduced.components_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1369,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49.99999999999999"
      ]
     },
     "execution_count": 1369,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scipy.linalg.norm(X_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1370,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.476735048214707e-16"
      ]
     },
     "execution_count": 1370,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orthogonality_constraint(pca_reduced.components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1397,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.64588048]\n",
      "12.709724140956093\n",
      "[2.64588048]\n",
      "13.201030603397726\n",
      "[2.64588048]\n",
      "12.963276706117949\n",
      "[2.64588048]\n",
      "13.185991116089346\n",
      "[2.64588048]\n",
      "12.747436119773973\n",
      "[2.64588048]\n",
      "12.74300642566949\n",
      "[2.64588048]\n",
      "12.564263198728224\n",
      "[2.64588048]\n",
      "12.267292115715172\n",
      "[2.64588048]\n",
      "12.182404385737996\n",
      "[2.64588048]\n",
      "11.701521759936078\n",
      "[2.64588048]\n",
      "11.902795531892481\n",
      "[2.64588048]\n",
      "11.396891784266412\n",
      "[2.64588048]\n",
      "11.581092321862945\n",
      "[2.64588048]\n",
      "11.137989398933316\n",
      "[2.64588048]\n",
      "10.399322938894295\n",
      "[2.64588048]\n",
      "10.300702974832447\n",
      "[2.64588048]\n",
      "9.25376777819446\n",
      "[2.64588048]\n",
      "9.501124883236852\n",
      "[2.64588048]\n",
      "9.1657655529231\n",
      "[2.64588048]\n",
      "8.048974879283152\n",
      "[2.64588048]\n",
      "8.008068670611651\n",
      "[2.64588048]\n",
      "7.109953014756209\n",
      "[2.64588048]\n",
      "5.706809012369791\n",
      "[2.64588048]\n",
      "5.998913528682459\n",
      "[2.64588048]\n",
      "4.558541534456868\n",
      "[2.64588048]\n",
      "5.006103733119091\n",
      "[2.64588048]\n",
      "3.9810819543199214\n",
      "[2.64588048]\n",
      "3.089415772542816\n",
      "[2.64588048]\n",
      "3.410065921910631\n",
      "[2.64588048]\n",
      "3.9607530245174325\n",
      "[2.64588048]\n",
      "3.524669433824351\n",
      "[2.64588048]\n",
      "3.0689102152583008\n",
      "[2.64588048]\n",
      "3.442814866454244\n",
      "[2.64588048]\n",
      "3.1466445199550996\n",
      "[2.64588048]\n",
      "2.941880545865499\n",
      "[2.64588048]\n",
      "2.7496841290449074\n",
      "[2.64588048]\n",
      "3.9827814257136596\n",
      "[2.64588048]\n",
      "3.083672323191565\n",
      "[2.64588048]\n",
      "3.6614347503884215\n",
      "[2.64588048]\n",
      "2.982981060238661\n",
      "[2.64588048]\n",
      "2.9211968250156306\n",
      "[2.64588048]\n",
      "4.322435871722493\n",
      "[2.64588048]\n",
      "2.6637126199692522\n",
      "[2.64588048]\n",
      "3.862905649041523\n",
      "[2.64588048]\n",
      "2.9640002541482238\n",
      "[2.64588048]\n",
      "3.1874495728680827\n",
      "[2.64588048]\n",
      "2.367450547194188\n",
      "[2.64588048]\n",
      "3.0741029299890243\n",
      "[2.64588048]\n",
      "3.6729899842161147\n",
      "[2.64588048]\n",
      "2.4662647128410056\n",
      "[2.64588048]\n",
      "3.140713876044552\n",
      "[2.64588048]\n",
      "3.1943810322695985\n",
      "[2.64588048]\n",
      "3.9409857477064696\n",
      "[2.64588048]\n",
      "4.842704091436438\n",
      "[2.64588048]\n",
      "2.5376749083882717\n",
      "[2.64588048]\n",
      "3.2866732101678453\n",
      "[2.64588048]\n",
      "2.981043888292328\n",
      "[2.64588048]\n",
      "2.6958033021755377\n",
      "[2.64588048]\n",
      "2.731314158308375\n",
      "[2.64588048]\n",
      "2.6867087488118626\n",
      "[2.64588048]\n",
      "3.428661137521985\n",
      "[2.64588048]\n",
      "2.8015829051941967\n",
      "[2.64588048]\n",
      "2.7407211522439456\n",
      "[2.64588048]\n",
      "3.66989583138597\n",
      "[2.64588048]\n",
      "3.247715626953722\n",
      "[2.64588048]\n",
      "3.031954899990635\n",
      "[2.64588048]\n",
      "3.0181767870550487\n",
      "[2.64588048]\n",
      "2.6407193021500945\n",
      "[2.64588048]\n",
      "2.6613223897413456\n",
      "[2.64588048]\n",
      "2.548292038533391\n",
      "[2.64588048]\n",
      "2.848151471787679\n",
      "[2.64588048]\n",
      "3.0395618320731677\n",
      "[2.64588048]\n",
      "2.467744980085516\n",
      "[2.64588048]\n",
      "2.5721707177017605\n",
      "[2.64588048]\n",
      "2.542778962843989\n",
      "[2.64588048]\n",
      "2.5670126148851864\n",
      "[2.64588048]\n",
      "2.6487905215177663\n",
      "[2.64588048]\n",
      "2.7833619485745276\n",
      "[2.64588048]\n",
      "2.502803950813105\n",
      "[2.64588048]\n",
      "2.445155581635578\n",
      "[2.64588048]\n",
      "2.607801693489412\n",
      "[2.64588048]\n",
      "2.848026962295191\n",
      "[2.64588048]\n",
      "2.9823824959145684\n",
      "[2.64588048]\n",
      "2.4899255189845544\n",
      "[2.64588048]\n",
      "2.341731595690767\n",
      "[2.64588048]\n",
      "2.6447707989197213\n",
      "[2.64588048]\n",
      "2.3905736456891207\n",
      "[2.64588048]\n",
      "2.2705932715639623\n",
      "[2.64588048]\n",
      "2.332747197789608\n",
      "[2.64588048]\n",
      "2.744613354491058\n",
      "[2.64588048]\n",
      "2.3256788913386037\n",
      "[2.64588048]\n",
      "2.5038034984247233\n",
      "[2.64588048]\n",
      "2.3377520621121572\n",
      "[2.64588048]\n",
      "2.5772181281249766\n",
      "[2.64588048]\n",
      "2.8668658115936543\n",
      "[2.64588048]\n",
      "2.1293660817856552\n",
      "[2.64588048]\n",
      "2.617210661023203\n",
      "[2.64588048]\n",
      "2.2340215189478063\n",
      "[2.64588048]\n",
      "2.402564653505942\n",
      "[2.64588048]\n",
      "2.505503162841815\n",
      "[2.64588048]\n",
      "2.45792532482782\n",
      "[2.64588048]\n",
      "2.3066974807046807\n",
      "[2.64588048]\n",
      "2.7459152335198684\n",
      "[2.64588048]\n",
      "3.0022126594413447\n",
      "[2.64588048]\n",
      "2.598314416743358\n",
      "[2.64588048]\n",
      "2.7007505796223934\n",
      "[2.64588048]\n",
      "2.1436770490102566\n",
      "[2.64588048]\n",
      "2.486059696655119\n",
      "[2.64588048]\n",
      "2.8614238496813114\n",
      "[2.64588048]\n",
      "2.2386648230105037\n",
      "[2.64588048]\n",
      "2.1023677616658185\n",
      "[2.64588048]\n",
      "2.816378996776892\n",
      "[2.64588048]\n",
      "2.532216158570629\n",
      "[2.64588048]\n",
      "2.846297854659714\n",
      "[2.64588048]\n",
      "3.2343343728293603\n",
      "[2.64588048]\n",
      "2.2693427188688458\n",
      "[2.64588048]\n",
      "2.4946764489829008\n",
      "[2.64588048]\n",
      "2.5671140551485925\n",
      "[2.64588048]\n",
      "3.076971340571108\n",
      "[2.64588048]\n",
      "3.3877721770211116\n",
      "[2.64588048]\n",
      "2.85588314850789\n",
      "[2.64588048]\n",
      "2.1910356624436336\n",
      "[2.64588048]\n",
      "2.1724118882279098\n",
      "[2.64588048]\n",
      "2.2314966931652958\n",
      "[2.64588048]\n",
      "2.1546104577349228\n",
      "[2.64588048]\n",
      "2.1728099585525182\n",
      "[2.64588048]\n",
      "2.6832263082442775\n",
      "[2.64588048]\n",
      "3.2836834048732144\n",
      "[2.64588048]\n",
      "3.116017657781568\n",
      "[2.64588048]\n",
      "2.1328808331790974\n",
      "[2.64588048]\n",
      "2.457163231962336\n",
      "[2.64588048]\n",
      "2.7762583514482753\n",
      "[2.64588048]\n",
      "1.650420442624555\n",
      "[2.64588048]\n",
      "2.6542677479151915\n",
      "[2.64588048]\n",
      "2.9422283603009007\n",
      "[2.64588048]\n",
      "2.229881734837894\n",
      "[2.64588048]\n",
      "2.8129425578093223\n",
      "[2.64588048]\n",
      "2.974777997053769\n",
      "[2.64588048]\n",
      "2.3220531098631767\n",
      "[2.64588048]\n",
      "3.0026774352278998\n",
      "[2.64588048]\n",
      "2.3757650718811076\n",
      "[2.64588048]\n",
      "2.277689223933916\n",
      "[2.64588048]\n",
      "2.6262941473477652\n",
      "[2.64588048]\n",
      "2.335079635877871\n",
      "[2.64588048]\n",
      "2.564943205672129\n",
      "[2.64588048]\n",
      "2.7233342271841137\n",
      "[2.64588048]\n",
      "2.8505505090576593\n",
      "[2.64588048]\n",
      "2.475618777983812\n",
      "[2.64588048]\n",
      "2.6109985433967\n",
      "[2.64588048]\n",
      "2.6824933240638957\n",
      "[2.64588048]\n",
      "2.380215044563681\n",
      "[2.64588048]\n",
      "2.6342635048011585\n",
      "[2.64588048]\n",
      "2.718924899255571\n",
      "[2.64588048]\n",
      "2.873701159484582\n",
      "[2.64588048]\n",
      "2.5393460628896753\n",
      "[2.64588048]\n",
      "2.633997345947482\n",
      "[2.64588048]\n",
      "2.6959113353172675\n",
      "[2.64588048]\n",
      "2.6130967426999105\n",
      "[2.64588048]\n",
      "2.495328442924554\n",
      "[2.64588048]\n",
      "2.656795262307031\n",
      "[2.64588048]\n",
      "2.5963722793421145\n",
      "[2.64588048]\n",
      "2.700549358701267\n",
      "[2.64588048]\n",
      "2.5760707054107135\n",
      "[2.64588048]\n",
      "2.6404597740014846\n",
      "[2.64588048]\n",
      "2.6160503739793692\n",
      "[2.64588048]\n",
      "2.6679307600877027\n",
      "[2.64588048]\n",
      "2.597620652015032\n",
      "[2.64588048]\n",
      "2.5454723343020538\n",
      "[2.64588048]\n",
      "2.636240997411414\n",
      "[2.64588048]\n",
      "2.6331400772965114\n",
      "[2.64588048]\n",
      "2.619000548846988\n",
      "[2.64588048]\n",
      "2.5823435140892883\n",
      "[2.64588048]\n",
      "2.6228433409021434\n",
      "[2.64588048]\n",
      "2.6401454472525923\n",
      "[2.64588048]\n",
      "2.6556744086233164\n",
      "[2.64588048]\n",
      "2.608447481471062\n",
      "[2.64588048]\n",
      "2.66580443409883\n",
      "[2.64588048]\n",
      "2.6135242904633773\n",
      "[2.64588048]\n",
      "2.622590967947618\n",
      "[2.64588048]\n",
      "2.62683959632593\n",
      "[2.64588048]\n",
      "2.644641313226068\n",
      "[2.64588048]\n",
      "2.618926233719028\n",
      "[2.64588048]\n",
      "2.6437264523339494\n",
      "[2.64588048]\n",
      "2.6643913367744156\n",
      "[2.64588048]\n",
      "2.6525800805733657\n",
      "[2.64588048]\n",
      "2.6219373827974937\n",
      "[2.64588048]\n",
      "2.6217595511289478\n",
      "[2.64588048]\n",
      "2.642236149612306\n",
      "[2.64588048]\n",
      "2.6510497642544704\n",
      "[2.64588048]\n",
      "2.6263767753468583\n",
      "[2.64588048]\n",
      "2.6453821254044043\n",
      "[2.64588048]\n",
      "2.657424946532911\n",
      "[2.64588048]\n",
      "2.637877807331393\n",
      "[2.64588048]\n",
      "2.6282425296160254\n",
      "[2.64588048]\n",
      "2.645983250857262\n",
      "[2.64588048]\n",
      "2.657935847016274\n",
      "[2.64588048]\n",
      "2.630964261028135\n",
      "[2.64588048]\n",
      "2.6425182113382397\n",
      "[2.64588048]\n",
      "2.640097940079043\n",
      "[2.64588048]\n",
      "2.6322895556978727\n",
      "[2.64588048]\n",
      "2.627306365210476\n",
      "[2.64588048]\n",
      "2.6450840126409756\n",
      "[2.64588048]\n",
      "2.6522390505529687\n",
      "[2.64588048]\n",
      "2.6329054325987644\n",
      "[2.64588048]\n",
      "2.64166140056407\n",
      "[2.64588048]\n",
      "2.636844968582785\n",
      "[2.64588048]\n",
      "2.653501666712713\n",
      "[2.64588048]\n",
      "2.6340068756386916\n",
      "[2.64588048]\n",
      "2.6358752618526053\n",
      "[2.64588048]\n",
      "2.6284669547797876\n",
      "[2.64588048]\n",
      "2.6399917628727856\n",
      "[2.64588048]\n",
      "2.6444339262156524\n",
      "[2.64588048]\n",
      "2.641811658621086\n",
      "[2.64588048]\n",
      "2.645743786780727\n",
      "[2.64588048]\n",
      "2.644012146056588\n",
      "[2.64588048]\n",
      "2.6462363006212253\n",
      "[2.64588048]\n",
      "2.6620633917555145\n",
      "[2.64588048]\n",
      "2.6366015817562882\n",
      "[2.64588048]\n",
      "2.641647301012779\n",
      "[2.64588048]\n",
      "2.6403514469039226\n",
      "[2.64588048]\n",
      "2.641317426432514\n",
      "[2.64588048]\n",
      "2.650065238401088\n",
      "[2.64588048]\n",
      "2.639918765193458\n",
      "[2.64588048]\n",
      "2.641309824157677\n",
      "[2.64588048]\n",
      "2.640508776025368\n",
      "[2.64588048]\n",
      "2.6363688040751585\n",
      "[2.64588048]\n",
      "2.641297211539319\n",
      "[2.64588048]\n",
      "2.6422644540564004\n",
      "[2.64588048]\n",
      "2.6345928084671026\n",
      "[2.64588048]\n",
      "2.6385452099028415\n",
      "[2.64588048]\n",
      "2.636378630833069\n",
      "[2.64588048]\n",
      "2.6419038351619677\n",
      "[2.64588048]\n",
      "2.637160593478835\n",
      "[2.64588048]\n",
      "2.638786149128199\n",
      "[2.64588048]\n",
      "2.640475524415021\n",
      "[2.64588048]\n",
      "2.641396334994992\n",
      "[2.64588048]\n",
      "2.637642369416218\n",
      "[2.64588048]\n",
      "2.64383170661132\n",
      "[2.64588048]\n",
      "2.6487738479785614\n",
      "[2.64588048]\n",
      "2.6388349659460886\n",
      "[2.64588048]\n",
      "2.6426654325686765\n",
      "[2.64588048]\n",
      "2.6432181351926545\n",
      "[2.64588048]\n",
      "2.647332785786965\n",
      "[2.64588048]\n",
      "2.6391101917258504\n",
      "[2.64588048]\n",
      "2.6413593878544566\n",
      "[2.64588048]\n",
      "2.641654558286281\n",
      "[2.64588048]\n",
      "2.645210266117353\n",
      "[2.64588048]\n",
      "2.642091574146665\n",
      "[2.64588048]\n",
      "2.646663360281249\n",
      "[2.64588048]\n",
      "2.645926506101458\n",
      "[2.64588048]\n",
      "2.6474866771842724\n",
      "[2.64588048]\n",
      "2.6430681308816624\n",
      "[2.64588048]\n",
      "2.641320220156808\n",
      "[2.64588048]\n",
      "2.645197741008212\n",
      "[2.64588048]\n",
      "2.642502652257047\n",
      "[2.64588048]\n",
      "2.6447176993665016\n",
      "[2.64588048]\n",
      "2.647742780499696\n",
      "[2.64588048]\n",
      "2.6430753115332934\n",
      "[2.64588048]\n",
      "2.6427085224028475\n",
      "[2.64588048]\n",
      "2.6441566321761094\n",
      "[2.64588048]\n",
      "2.64370430364668\n",
      "[2.64588048]\n",
      "2.645209557327933\n",
      "[2.64588048]\n",
      "2.6432981127445365\n",
      "[2.64588048]\n",
      "2.64486929952768\n",
      "[2.64588048]\n",
      "2.645803874013584\n",
      "[2.64588048]\n",
      "2.6461732664681037\n",
      "[2.64588048]\n",
      "2.643767035801695\n",
      "[2.64588048]\n",
      "2.6441092043164334\n",
      "[2.64588048]\n",
      "2.645243762747819\n",
      "[2.64588048]\n",
      "2.6462033856687617\n",
      "[2.64588048]\n",
      "2.6462901037591076\n",
      "[2.64588048]\n",
      "2.6468381771550398\n",
      "[2.64588048]\n",
      "2.6445292429973795\n",
      "[2.64588048]\n",
      "2.643603688471345\n",
      "[2.64588048]\n",
      "2.6456157940167393\n",
      "[2.64588048]\n",
      "2.6464867449431946\n",
      "[2.64588048]\n",
      "2.644669184531976\n",
      "[2.64588048]\n",
      "2.645128237286806\n",
      "[2.64588048]\n",
      "2.645168465500658\n",
      "[2.64588048]\n",
      "2.6460235865024084\n",
      "[2.64588048]\n",
      "2.646446515975772\n",
      "[2.64588048]\n",
      "2.6451053766343793\n",
      "[2.64588048]\n",
      "2.644706585675489\n",
      "[2.64588048]\n",
      "2.6456929320271114\n",
      "[2.64588048]\n",
      "2.645127420562932\n",
      "[2.64588048]\n",
      "2.6455545963847404\n",
      "[2.64588048]\n",
      "2.6456883087596386\n",
      "[2.64588048]\n",
      "2.6458893265739216\n",
      "[2.64588048]\n",
      "2.6462724048946416\n",
      "[2.64588048]\n",
      "2.6453896396773215\n",
      "[2.64588048]\n",
      "2.6457465319241606\n",
      "[2.64588048]\n",
      "2.6458555447965457\n",
      "[2.64588048]\n",
      "2.6460853346651776\n",
      "[2.64588048]\n",
      "2.645562927463574\n",
      "[2.64588048]\n",
      "2.6458245134318794\n",
      "[2.64588048]\n",
      "2.6455677383597798\n",
      "[2.64588048]\n",
      "2.6457896647233934\n",
      "[2.64588048]\n",
      "2.6459468618062063\n",
      "[2.64588048]\n",
      "2.6456539151630114\n",
      "[2.64588048]\n",
      "2.6457968181977556\n",
      "[2.64588048]\n",
      "2.6457072047104253\n",
      "[2.64588048]\n",
      "2.6458405083405587\n",
      "[2.64588048]\n",
      "2.6459387944211232\n",
      "[2.64588048]\n",
      "2.645894403355784\n",
      "[2.64588048]\n",
      "2.6460498365634595\n",
      "[2.64588048]\n",
      "2.645720986673307\n",
      "[2.64588048]\n",
      "2.645870791597446\n",
      "[2.64588048]\n",
      "2.645765011225362\n",
      "[2.64588048]\n",
      "2.6457845289429076\n",
      "[2.64588048]\n",
      "2.645939221021849\n",
      "[2.64588048]\n",
      "2.645772004412871\n",
      "[2.64588048]\n",
      "2.645881121587142\n",
      "[2.64588048]\n",
      "2.645939221021849\n",
      "[2.64588048]\n",
      "2.6458227823659826\n",
      "[2.64588048]\n",
      "2.645846231088121\n",
      "[2.64588048]\n",
      "2.6459050167968132\n",
      "[2.64588048]\n",
      "2.6458126661456585\n",
      "[2.64588048]\n",
      "2.6459013412353305\n",
      "[2.64588048]\n",
      "2.6458204457555077\n",
      "[2.64588048]\n",
      "2.6458226790483472\n",
      "[2.64588048]\n",
      "2.64580760564497\n",
      "[2.64588048]\n",
      "2.6458641151432554\n",
      "[2.64588048]\n",
      "2.645892529571466\n",
      "[2.64588048]\n",
      "2.6458737596174147\n",
      "[2.64588048]\n",
      "2.6458609034780607\n",
      "[2.64588048]\n",
      "2.645873339173025\n",
      "[2.64588048]\n",
      "2.645887200373858\n",
      "[2.64588048]\n",
      "2.6458364971508037\n",
      "[2.64588048]\n",
      "2.6458213682586345\n",
      "[2.64588048]\n",
      "2.645898718778998\n",
      "[2.64588048]\n",
      "2.645829076239735\n",
      "[2.64588048]\n",
      "2.6458134145769363\n",
      "[2.64588048]\n",
      "2.6457862706726436\n",
      "[2.64588048]\n",
      "2.6458003433074344\n",
      "[2.64588048]\n",
      "2.6458254012254674\n",
      "[2.64588048]\n",
      "2.6458306425211324\n",
      "[2.64588048]\n",
      "2.6458226743321958\n",
      "[2.64588048]\n",
      "2.6458296789927003\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.64588048]\n",
      "2.645827107755051\n",
      "[2.64588048]\n",
      "2.6457485559213922\n",
      "[2.64588048]\n",
      "2.645689640769719\n",
      "[2.64588048]\n",
      "2.645775110888916\n",
      "[2.64588048]\n",
      "2.64576468407889\n",
      "[2.64588048]\n",
      "2.6458025520350748\n",
      "[2.64588048]\n",
      "2.6457361975538727\n",
      "[2.64588048]\n",
      "2.645701980501129\n",
      "[2.64588048]\n",
      "2.6457127118079686\n",
      "[2.64588048]\n",
      "2.6456276599673525\n",
      "[2.64588048]\n",
      "2.6455327859512017\n",
      "[2.64588048]\n",
      "2.6457173691194553\n",
      "[2.64588048]\n",
      "2.6455543728102953\n",
      "[2.64588048]\n",
      "2.645434420880188\n",
      "[2.64588048]\n",
      "2.6454791408152984\n",
      "[2.64588048]\n",
      "2.645479016485007\n",
      "[2.64588048]\n",
      "2.6453693948146153\n",
      "[2.64588048]\n",
      "2.645246516914669\n",
      "[2.64588048]\n",
      "2.645123524306527\n",
      "[2.64588048]\n",
      "2.645168972520307\n",
      "[2.64588048]\n",
      "2.645026495475292\n",
      "[2.64588048]\n",
      "2.6450173536933237\n",
      "[2.64588048]\n",
      "2.6448116664555363\n",
      "[2.64588048]\n",
      "2.6447719828423004\n",
      "[2.64588048]\n",
      "2.6444820469789847\n",
      "[2.64588048]\n",
      "2.6445224243073855\n",
      "[2.64588048]\n",
      "2.6445446204606573\n",
      "[2.64588048]\n",
      "2.6443955434777915\n",
      "[2.64588048]\n",
      "2.6440846632588393\n",
      "[2.64588048]\n",
      "2.643708675349552\n",
      "[2.64588048]\n",
      "2.6437500485375227\n",
      "[2.64588048]\n",
      "2.643774093065215\n",
      "[2.64588048]\n",
      "2.6435050349649067\n",
      "[2.64588048]\n",
      "2.6437325054703282\n",
      "[2.64588048]\n",
      "2.644052627589374\n",
      "[2.64588048]\n",
      "2.6440901275768702\n",
      "[2.64588048]\n",
      "2.6446293008511015\n",
      "[2.64588048]\n",
      "2.64370682133212\n",
      "[2.64588048]\n",
      "2.6432923717367727\n",
      "[2.64588048]\n",
      "2.6428041597119485\n",
      "[2.64588048]\n",
      "2.6436059553649014\n",
      "[2.64588048]\n",
      "2.6436765057162765\n",
      "[2.64588048]\n",
      "2.6434165033648505\n",
      "[2.64588048]\n",
      "2.6437583452435653\n",
      "[2.64588048]\n",
      "2.6429356323843782\n",
      "[2.64588048]\n",
      "2.6424420076031434\n",
      "[2.64588048]\n",
      "2.643027062961595\n",
      "[2.64588048]\n",
      "2.642794559557443\n",
      "[2.64588048]\n",
      "2.642666675953851\n",
      "[2.64588048]\n",
      "2.642185031636839\n",
      "[2.64588048]\n",
      "2.64169658135285\n",
      "[2.64588048]\n",
      "2.642129904233287\n",
      "[2.64588048]\n",
      "2.6419464310273297\n",
      "[2.64588048]\n",
      "2.6417876345794715\n",
      "[2.64588048]\n",
      "2.64257438185297\n",
      "[2.64588048]\n",
      "2.6418733341304557\n",
      "[2.64588048]\n",
      "2.641553695197589\n",
      "[2.64588048]\n",
      "2.6421258767176337\n",
      "[2.64588048]\n",
      "2.6422364728867587\n",
      "[2.64588048]\n",
      "2.642763887156756\n",
      "[2.64588048]\n",
      "2.641609237012076\n",
      "[2.64588048]\n",
      "2.64107117097736\n",
      "[2.64588048]\n",
      "2.641550022028795\n",
      "[2.64588048]\n",
      "2.641833627551077\n",
      "[2.64588048]\n",
      "2.640635231051901\n",
      "[2.64588048]\n",
      "2.6422227555104763\n",
      "[2.64588048]\n",
      "2.642212878283428\n",
      "[2.64588048]\n",
      "2.6415957523830493\n",
      "[2.64588048]\n",
      "2.64138854246813\n",
      "[2.64588048]\n",
      "2.6418189203071076\n",
      "[2.64588048]\n",
      "2.6412331063506667\n",
      "[2.64588048]\n",
      "2.6419279968238873\n",
      "[2.64588048]\n",
      "2.6410070489223023\n",
      "[2.64588048]\n",
      "2.64191475845741\n",
      "[2.64588048]\n",
      "2.641901992522536\n",
      "[2.64588048]\n",
      "2.641989496074126\n",
      "[2.64588048]\n",
      "2.64194867094669\n",
      "[2.64588048]\n",
      "2.6416845415669763\n",
      "[2.64588048]\n",
      "2.6416157319462887\n",
      "[2.64588048]\n",
      "2.641689919077384\n",
      "[2.64588048]\n",
      "2.6414115619669087\n",
      "[2.64588048]\n",
      "2.6415327304783878\n",
      "[2.64588048]\n",
      "2.641642095063069\n",
      "[2.64588048]\n",
      "2.641718741238254\n",
      "[2.64588048]\n",
      "2.6413294804817946\n",
      "[2.64588048]\n",
      "2.6417244020637423\n",
      "[2.64588048]\n",
      "2.6414865032929375\n",
      "[2.64588048]\n",
      "2.6416136673506694\n",
      "[2.64588048]\n",
      "2.6414828109825907\n",
      "[2.64588048]\n",
      "2.6419019122406966\n",
      "[2.64588048]\n",
      "2.641639694342725\n",
      "[2.64588048]\n",
      "2.6416692782689055\n",
      "[2.64588048]\n",
      "2.6413081798548403\n",
      "[2.64588048]\n",
      "2.641731559136402\n",
      "[2.64588048]\n",
      "2.6420020648189753\n",
      "[2.64588048]\n",
      "2.641536711577533\n",
      "[2.64588048]\n",
      "2.6416452855097874\n",
      "[2.64588048]\n",
      "2.641553723095329\n",
      "[2.64588048]\n",
      "2.6417396959219803\n",
      "[2.64588048]\n",
      "2.6414938454990895\n",
      "[2.64588048]\n",
      "2.6413788469921284\n",
      "[2.64588048]\n",
      "2.6417443935861518\n",
      "[2.64588048]\n",
      "2.6415444955570058\n",
      "[2.64588048]\n",
      "2.6416661281141405\n",
      "[2.64588048]\n",
      "2.641505715936895\n",
      "[2.64588048]\n",
      "2.6416635023520274\n",
      "[2.64588048]\n",
      "2.641623892228789\n",
      "[2.64588048]\n",
      "2.6416978165169955\n",
      "[2.64588048]\n",
      "2.641581702635166\n",
      "[2.64588048]\n",
      "2.641661551162443\n",
      "[2.64588048]\n",
      "2.6416098029644624\n",
      "[2.64588048]\n",
      "2.641528875548981\n",
      "[2.64588048]\n",
      "2.6414856001336835\n",
      "[2.64588048]\n",
      "2.641417450513948\n",
      "[2.64588048]\n",
      "2.6413081179567337\n",
      "[2.64588048]\n",
      "2.641430059963473\n",
      "[2.64588048]\n",
      "2.6413344284154556\n",
      "[2.64588048]\n",
      "2.6415372542344167\n",
      "[2.64588048]\n",
      "2.641399566959914\n",
      "[2.64588048]\n",
      "2.6415019061425924\n",
      "[2.64588048]\n",
      "2.641538965762776\n",
      "[2.64588048]\n",
      "2.641455345292856\n",
      "[2.64588048]\n",
      "2.6414931737584673\n",
      "[2.64588048]\n",
      "2.6414825239618724\n",
      "[2.64588048]\n",
      "2.641410207139285\n",
      "[2.64588048]\n",
      "2.641498188638036\n",
      "[2.64588048]\n",
      "2.641459824955044\n",
      "[2.64588048]\n",
      "2.64142194043504\n",
      "[2.64588048]\n",
      "2.641385418608734\n",
      "[2.64588048]\n",
      "2.6414621649435763\n",
      "[2.64588048]\n",
      "2.6414552706181342\n",
      "[2.64588048]\n",
      "2.6414215732601\n",
      "[2.64588048]\n",
      "2.6413715172883805\n",
      "[2.64588048]\n",
      "2.6414653831220773\n",
      "[2.64588048]\n",
      "2.6414691422641337\n",
      "[2.64588048]\n",
      "2.641496111887705\n",
      "[2.64588048]\n",
      "2.6414386822986566\n",
      "[2.64588048]\n",
      "2.6414419512449343\n",
      "[2.64588048]\n",
      "2.6414311877561887\n",
      "[2.64588048]\n",
      "2.6414491696581615\n",
      "[2.64588048]\n",
      "2.641481015992962\n",
      "[2.64588048]\n",
      "2.6414644468458968\n",
      "[2.64588048]\n",
      "2.641433401609423\n",
      "[2.64588048]\n",
      "2.641419102495445\n",
      "[2.64588048]\n",
      "2.6414472596088467\n",
      "[2.64588048]\n",
      "2.6414397868329687\n",
      "[2.64588048]\n",
      "2.641437608839782\n",
      "[2.64588048]\n",
      "2.6414119246779046\n",
      "[2.64588048]\n",
      "2.641386875923926\n",
      "[2.64588048]\n",
      "2.6414508222613695\n",
      "[2.64588048]\n",
      "2.641466986544433\n",
      "[2.64588048]\n",
      "2.641403116819846\n",
      "[2.64588048]\n",
      "2.6413806016816124\n",
      "[2.64588048]\n",
      "2.641402481320985\n",
      "[2.64588048]\n",
      "2.641384339854259\n",
      "[2.64588048]\n",
      "2.6414172696619365\n",
      "[2.64588048]\n",
      "2.6413891933892146\n",
      "[2.64588048]\n",
      "2.6413698992372225\n",
      "[2.64588048]\n",
      "2.641400594418807\n",
      "[2.64588048]\n",
      "2.6413503059846066\n",
      "[2.64588048]\n",
      "2.641365592989201\n",
      "[2.64588048]\n",
      "2.641373745042073\n",
      "[2.64588048]\n",
      "2.641355730412935\n",
      "[2.64588048]\n",
      "2.6413259902782844\n",
      "[2.64588048]\n",
      "2.64137803037393\n",
      "[2.64588048]\n",
      "2.6414310920445097\n",
      "[2.64588048]\n",
      "2.641346534157657\n",
      "[2.64588048]\n",
      "2.641344060318197\n",
      "[2.64588048]\n",
      "2.6413269562943498\n",
      "[2.64588048]\n",
      "2.641363827650227\n",
      "[2.64588048]\n",
      "2.641355813104932\n",
      "[2.64588048]\n",
      "2.6413451479449264\n",
      "[2.64588048]\n",
      "2.6413627704205935\n",
      "[2.64588048]\n",
      "2.641370689432596\n",
      "[2.64588048]\n",
      "2.641338055436275\n",
      "[2.64588048]\n",
      "2.641329894842637\n",
      "[2.64588048]\n",
      "2.6413108009369437\n",
      "[2.64588048]\n",
      "2.641333721245027\n",
      "[2.64588048]\n",
      "2.6413188919275865\n",
      "[2.64588048]\n",
      "2.6413364223705336\n",
      "[2.64588048]\n",
      "2.641361390034665\n",
      "[2.64588048]\n",
      "2.6413055625855133\n",
      "[2.64588048]\n",
      "2.641328120392831\n",
      "[2.64588048]\n",
      "2.6413363763929985\n",
      "[2.64588048]\n",
      "2.6413288604056606\n",
      "[2.64588048]\n",
      "2.6412951662689914\n",
      "[2.64588048]\n",
      "2.641342427234497\n",
      "[2.64588048]\n",
      "2.6413614357176813\n",
      "[2.64588048]\n",
      "2.641318765271118\n",
      "[2.64588048]\n",
      "2.641332205348526\n",
      "[2.64588048]\n",
      "2.6413296220171194\n",
      "[2.64588048]\n",
      "2.6413337521745355\n",
      "[2.64588048]\n",
      "2.6413293954426287\n",
      "[2.64588048]\n",
      "2.6413272183838425\n",
      "[2.64588048]\n",
      "2.641332530909066\n",
      "[2.64588048]\n",
      "2.641314167540513\n",
      "[2.64588048]\n",
      "2.6413208048005394\n",
      "[2.64588048]\n",
      "2.6413223213524315\n",
      "[2.64588048]\n",
      "2.6413277764610448\n",
      "[2.64588048]\n",
      "2.6413190194998073\n",
      "[2.64588048]\n",
      "2.6413286498740955\n",
      "[2.64588048]\n",
      "2.641322681070482\n",
      "[2.64588048]\n",
      "2.6413271524356214\n",
      "[2.64588048]\n",
      "2.641319842578458\n",
      "[2.64588048]\n",
      "2.641315281560138\n",
      "[2.64588048]\n",
      "2.64131820824039\n",
      "[2.64588048]\n",
      "2.6413253561917087\n",
      "[2.64588048]\n",
      "2.641327065448265\n",
      "[2.64588048]\n",
      "2.641322323881314\n",
      "[2.64588048]\n",
      "2.6413306048544576\n",
      "[2.64588048]\n",
      "2.6413216594554108\n",
      "[2.64588048]\n",
      "2.641322373463871\n",
      "[2.64588048]\n",
      "2.6413231124740717\n",
      "[2.64588048]\n",
      "2.6413257543519975\n",
      "[2.64588048]\n",
      "2.641323164889713\n",
      "[2.64588048]\n",
      "2.6413196818027167\n",
      "[2.64588048]\n",
      "2.6413251571187404\n",
      "[2.64588048]\n",
      "2.641316894049545\n",
      "[2.64588048]\n",
      "2.6413240795723545\n",
      "[2.64588048]\n",
      "2.6413230142059514\n",
      "[2.64588048]\n",
      "2.641329962256964\n",
      "[2.64588048]\n",
      "2.641319999394742\n",
      "[2.64588048]\n",
      "2.6413219993566193\n",
      "[2.64588048]\n",
      "2.641321446576751\n",
      "[2.64588048]\n",
      "2.6413187646765364\n",
      "[2.64588048]\n",
      "2.6413191708142962\n",
      "[2.64588048]\n",
      "2.641322051103698\n",
      "[2.64588048]\n",
      "2.6413230864856923\n",
      "[2.64588048]\n",
      "2.641320743383624\n",
      "[2.64588048]\n",
      "2.641321063204474\n",
      "[2.64588048]\n",
      "2.641320204609867\n",
      "[2.64588048]\n",
      "2.6413215867163053\n",
      "[2.64588048]\n",
      "2.6413218443976283\n",
      "[2.64588048]\n",
      "2.6413251253451158\n",
      "[2.64588048]\n",
      "2.6413203314551965\n",
      "[2.64588048]\n",
      "2.6413217397357935\n",
      "[2.64588048]\n",
      "2.641321623845244\n",
      "[2.64588048]\n",
      "2.64132139270942\n",
      "[2.64588048]\n",
      "2.6413217308315957\n",
      "[2.64588048]\n",
      "2.641322505739356\n",
      "[2.64588048]\n",
      "2.641321402439162\n"
     ]
    }
   ],
   "source": [
    "# minimize(reconstruction_error, x0 = pca_reduced.components_.flatten())\n",
    "# result = minimize(reconstruction_error, x0 = np.random.normal(0, 1, len(pca_reduced.components_.flatten())))\n",
    "# result = minimize(reconstruction_error, x0 = np.random.normal(0, 1, 5))\n",
    "# result = minimize(reconstruction_error, x0 = np.zeros(5), method='Nelder-Mead')\n",
    "result = minimize(reconstruction_error, x0 = np.random.normal(0, 1, 5), method='Nelder-Mead')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1398,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.58947845,  0.0956696 ,  0.58970625, -0.54332439, -0.02019066])"
      ]
     },
     "execution_count": 1398,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1399,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.9645703891745256e-08"
      ]
     },
     "execution_count": 1399,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orthogonality_constraint(result.x.reshape(-1, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1400,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.00000003])"
      ]
     },
     "execution_count": 1400,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(result.x.reshape(-1, 5) ** 2, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1401,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.58947845,  0.0956696 ,  0.58970625, -0.54332439, -0.02019066]])"
      ]
     },
     "execution_count": 1401,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.x.reshape(-1, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1402,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.64588048]\n",
      "2.6413217308315957\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.004558749390316308"
      ]
     },
     "execution_count": 1402,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_variance(result.x.reshape(-1, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1396,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " final_simplex: (array([[ 0.57537977, -0.08233875, -0.57579062,  0.57499768,  0.00113708],\n",
       "       [ 0.57543113, -0.08231884, -0.5758173 ,  0.57492259,  0.00105158],\n",
       "       [ 0.57541779, -0.08233055, -0.57573594,  0.57501552,  0.00116234],\n",
       "       [ 0.57539854, -0.08231482, -0.57573352,  0.57503957,  0.00109954],\n",
       "       [ 0.57546944, -0.08238281, -0.57576588,  0.57492643,  0.00111109],\n",
       "       [ 0.57546096, -0.08228142, -0.57577221,  0.57494322,  0.00105704]]), array([2.55828653, 2.55828654, 2.55828654, 2.55828654, 2.55828656,\n",
       "       2.55828656]))\n",
       "           fun: 2.558286531515811\n",
       "       message: 'Optimization terminated successfully.'\n",
       "          nfev: 850\n",
       "           nit: 541\n",
       "        status: 0\n",
       "       success: True\n",
       "             x: array([ 0.57537977, -0.08233875, -0.57579062,  0.57499768,  0.00113708])"
      ]
     },
     "execution_count": 1396,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = pca_reduced.components_.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.57542321 -0.0823935  -0.57578763  0.57494914  0.00123692 -0.21854994\n",
      " -0.71220901  0.01300828  0.13110063 -0.6539401 ]\n"
     ]
    }
   ],
   "source": [
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.57542321, -0.0823935 , -0.57578763,  0.57494914,  0.00123692],\n",
       "       [-0.21854994, -0.71220901,  0.01300828,  0.13110063, -0.6539401 ]])"
      ]
     },
     "execution_count": 492,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca_reduced.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "metadata": {},
   "outputs": [],
   "source": [
    "V1 = x.reshape(-1, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1.])"
      ]
     },
     "execution_count": 540,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(V1 ** 2, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 541,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "V2 = result.x.reshape(-1, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 566,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.01719766, -0.01719765])"
      ]
     },
     "execution_count": 566,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(V2 ** 2, axis = 1) - np.ones(V2.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 560,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.024321158042332734"
      ]
     },
     "execution_count": 560,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scipy.linalg.norm(np.sum(V2 ** 2, axis = 1) - np.ones(V2.shape[0]), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 571,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.024321156942014498"
      ]
     },
     "execution_count": 571,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sqrt(0.01719766 ** 2 + (0.01719765 ** 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 563,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = np.array([1, 2, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 568,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.7416573867739413"
      ]
     },
     "execution_count": 568,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scipy.linalg.norm(a, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 569,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.7416573867739413"
      ]
     },
     "execution_count": 569,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sqrt((1 ** 2 + 2 ** 2 + 3 ** 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 562,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0002957595094756"
      ]
     },
     "execution_count": 562,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.01719766 ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 570,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.00029575916552249993"
      ]
     },
     "execution_count": 570,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(-0.01719765 ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.linalg.eig(cc)[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "keras_tf",
   "language": "python",
   "name": "keras_tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
