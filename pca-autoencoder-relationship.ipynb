{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "from sklearn import datasets\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn import decomposition\n",
    "import scipy\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input, Dense, Layer, InputSpec\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "from keras import regularizers, activations, initializers, constraints, Sequential\n",
    "from keras import backend as K\n",
    "from keras.constraints import UnitNorm, Constraint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate random multi-dimensional correlated data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 1**. Set the dimension of the data.\n",
    "\n",
    "We set the dim small to clear understanding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_dim = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2.1.** Generate a positive definite symmetric matrix to be used as covariance to generate a random data.\n",
    "\n",
    "This is a matrix of size n_dim x n_dim."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cov = sklearn.datasets.make_spd_matrix(n_dim, random_state=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2.2.** Generate a vector of mean for generating the random data.\n",
    "\n",
    "This is an np array of size n_dim."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mu = np.random.normal(0, 0.1, n_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3**. Generate the random data, `X`.\n",
    "\n",
    "The number of samples for `X` is set as `n`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n = 1000\n",
    "\n",
    "X = np.random.multivariate_normal(mu, cov, n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 4.** Split the data into train and test.\n",
    "\n",
    "We split the data into train and test. The test will be used to measure the improvement in Autoencoder after tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test = train_test_split(X, test_size=0.5, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.52742251, -0.5274438 ,  0.23408997,  1.81206222, -0.35162448],\n",
       "       [-0.5274438 ,  0.55208126, -0.01248374, -0.72039515,  0.10768684],\n",
       "       [ 0.23408997, -0.01248374,  0.61008193,  0.04866543, -0.16359657],\n",
       "       [ 1.81206222, -0.72039515,  0.04866543,  4.73651825, -0.53528873],\n",
       "       [-0.35162448,  0.10768684, -0.16359657, -0.53528873,  0.75331836]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.cov(X_train.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# scaler = StandardScaler().fit(X_train)\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X_train)\n",
    "\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "X_test_scaled = scaler.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.28802   , 0.72403075, 0.4411856 , 0.41454305, 0.54474074],\n",
       "       [0.31318938, 0.70208478, 0.24933067, 0.48874311, 0.6244617 ],\n",
       "       [0.5826303 , 0.42574537, 0.55908206, 0.61234947, 0.53454499],\n",
       "       ...,\n",
       "       [0.34993009, 0.50725329, 0.47272284, 0.36309813, 0.36297567],\n",
       "       [0.25755915, 0.63645586, 0.26228602, 0.33809652, 0.58367054],\n",
       "       [0.54259375, 0.6178697 , 0.37484038, 0.4000808 , 0.57805756]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA vs Single Layer Linear Autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit Principal Component Analysis (PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PCA(copy=True, iterated_power='auto', n_components=2, random_state=None,\n",
       "  svd_solver='auto', tol=0.0, whiten=False)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca = decomposition.PCA(n_components=2)\n",
    "\n",
    "pca.fit(X_train_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit Single Layer Linear Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/inferno/virtualenvs/keras_tf/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 2)                 12        \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 5)                 15        \n",
      "=================================================================\n",
      "Total params: 27\n",
      "Trainable params: 27\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:From /Users/inferno/virtualenvs/keras_tf/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1098c4668>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_epoch = 100\n",
    "batch_size = 16\n",
    "input_dim = X_train_scaled.shape[1] #num of predictor variables, \n",
    "encoding_dim = 2\n",
    "learning_rate = 1e-3\n",
    "\n",
    "encoder = Dense(encoding_dim, activation=\"linear\", input_shape=(input_dim,), use_bias = True) \n",
    "decoder = Dense(input_dim, activation=\"linear\", use_bias = True)\n",
    "\n",
    "autoencoder = Sequential()\n",
    "autoencoder.add(encoder)\n",
    "autoencoder.add(decoder)\n",
    "\n",
    "autoencoder.compile(metrics=['accuracy'],\n",
    "                    loss='mean_squared_error',\n",
    "                    optimizer='sgd')\n",
    "autoencoder.summary()\n",
    "\n",
    "autoencoder.fit(X_train_scaled, X_train_scaled,\n",
    "                epochs=nb_epoch,\n",
    "                batch_size=batch_size,\n",
    "                shuffle=True,\n",
    "                verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare and contrast the outputs.\n",
    "\n",
    "### 1. Tied Weights\n",
    "\n",
    "The weights on Encoder and Decoder are not the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder weights \n",
      " [[ 0.34 -0.68  0.5   0.06  0.74]\n",
      " [-0.28  0.54 -0.71  0.34  0.34]]\n",
      "Decoder weights \n",
      " [[-0.16 -0.55 -0.05  0.12  0.7 ]\n",
      " [-0.61  0.09 -0.68  0.44  0.78]]\n"
     ]
    }
   ],
   "source": [
    "w_encoder = np.round(autoencoder.layers[0].get_weights()[0], 2).T  # W in Figure 2.\n",
    "w_decoder = np.round(autoencoder.layers[1].get_weights()[0], 2)  # W' in Figure 2.\n",
    "print('Encoder weights \\n', w_encoder)\n",
    "print('Decoder weights \\n', w_decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Weight Orthogonality\n",
    "Unlike PCA weights, the weights on Encoder and Decoder are not orthogonal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1., -0.],\n",
       "       [-0.,  1.]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_pca = pca.components_\n",
    "np.round(np.dot(w_pca, w_pca.T), 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.379, -0.545],\n",
       "       [-0.545,  1.105]], dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(np.dot(w_encoder, w_encoder.T), 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.835, 0.681],\n",
       "       [0.681, 1.645]], dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(np.dot(w_decoder, w_decoder.T), 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Uncorrelated Features\n",
    "Unlike PCA features, i.e. Principal Scores, the Encoded features are correlated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.07356, 0.     ],\n",
       "       [0.     , 0.02691]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca_features = pca.fit_transform(X_train_scaled)\n",
    "np.round(np.cov(pca_features.T), 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded feature covariance\n",
      " [[ 0.03769361 -0.02185375]\n",
      " [-0.02185375  0.03382182]]\n"
     ]
    }
   ],
   "source": [
    "encoder_layer = Model(inputs=autoencoder.inputs, outputs=autoencoder.layers[0].output)\n",
    "encoded_features = np.array(encoder_layer.predict(X_train_scaled))\n",
    "print('Encoded feature covariance\\n', np.cov(encoded_features.T))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Unit Norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA weights norm, \n",
      " [1. 1.]\n",
      "Encoder weights norm, \n",
      " [1.3792 1.1053]\n",
      "Decoder weights norm, \n",
      " [0.835     1.6445999]\n"
     ]
    }
   ],
   "source": [
    "print('PCA weights norm, \\n', np.sum(w_pca ** 2, axis = 1))\n",
    "print('Encoder weights norm, \\n', np.sum(w_encoder ** 2, axis = 1))\n",
    "print('Decoder weights norm, \\n', np.sum(w_decoder ** 2, axis = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.01280309, 0.00732339],\n",
       "       [0.00732339, 1.0018437 ]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(np.dot(w_pca_noisy, w_pca_noisy.T), 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.07449593, 0.00049422],\n",
       "       [0.00049422, 0.02696547]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_pca_features_noisy = np.dot(X_train_scaled, w_pca.T)\n",
    "np.round(np.cov(X_train_pca_features_noisy.T), 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The features are orthogonal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 7.35579672e-02, -9.79000000e-18],\n",
       "       [-9.79000000e-18,  2.69149908e-02]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(np.cov(X_train_pca_features.T), 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The PCA transform weights are orthogonal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.01280309, 0.00732339],\n",
       "       [0.00732339, 1.0018437 ]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(np.dot(w_pca, w_pca.T), 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The PCA transform weights have unit norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.01280309, 1.0018437 ])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(w_pca ** 2, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.64515469, -0.51416254,  0.12819653,  0.51003097, -0.23590138],\n",
       "       [ 0.09718476,  0.38589638,  0.79342538, -0.12496673, -0.44535635]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.07, 0.03])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eigenvalues = np.round(pca.explained_variance_, 2)\n",
    "eigenvalues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plain vanilla PCA to mimic PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_3 (Dense)              (None, 2)                 12        \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 5)                 15        \n",
      "=================================================================\n",
      "Total params: 27\n",
      "Trainable params: 27\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x135326cc0>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_epoch = 100\n",
    "batch_size = 16\n",
    "input_dim = X_train_scaled.shape[1] #num of predictor variables, \n",
    "encoding_dim = 2\n",
    "learning_rate = 1e-3\n",
    "\n",
    "# input_layer = Input(shape=(input_dim, ))\n",
    "# encoder = Dense(encoding_dim, activation=\"linear\", use_bias = False)(input_layer)\n",
    "# decoder = Dense(input_dim, activation=\"linear\", use_bias = False)(encoder)\n",
    "# autoencoder = Model(inputs=input_layer, outputs=decoder)\n",
    "# autoencoder.summary()\n",
    "\n",
    "encoder = Dense(encoding_dim, activation=\"linear\", input_shape=(input_dim,), use_bias = True) \n",
    "decoder = Dense(input_dim, activation=\"linear\", use_bias = True)\n",
    "\n",
    "autoencoder = Sequential()\n",
    "autoencoder.add(encoder)\n",
    "autoencoder.add(decoder)\n",
    "\n",
    "autoencoder.compile(metrics=['accuracy'],\n",
    "                    loss='mean_squared_error',\n",
    "                    optimizer='sgd')\n",
    "autoencoder.summary()\n",
    "\n",
    "autoencoder.fit(X_train_scaled, X_train_scaled,\n",
    "                epochs=nb_epoch,\n",
    "                batch_size=batch_size,\n",
    "                shuffle=True,\n",
    "                verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.02, -0.26,  0.77, -0.71,  0.58],\n",
       "       [ 0.22,  0.47, -0.88, -0.54, -0.23]], dtype=float32)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_encoder = np.round(autoencoder.layers[0].get_weights()[0], 2).T\n",
    "w_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.51,  0.56,  0.4 , -0.69, -0.19],\n",
       "       [-0.86,  0.51, -0.68, -0.81,  0.21]], dtype=float32)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_decoder = np.round(autoencoder.layers[1].get_weights()[0], 2)\n",
    "w_decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train reconstrunction error\n",
      " 0.01691868241927651\n",
      "Test reconstrunction error\n",
      " 0.0186566660959673\n"
     ]
    }
   ],
   "source": [
    "train_predictions = autoencoder.predict(X_train_scaled)\n",
    "print('Train reconstrunction error\\n', sklearn.metrics.mean_squared_error(X_train_scaled, train_predictions))\n",
    "test_predictions = autoencoder.predict(X_test_scaled)\n",
    "print('Test reconstrunction error\\n', sklearn.metrics.mean_squared_error(X_test_scaled, test_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The weights are not orthogonal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder weights\n",
      " [[ 1.501 -0.545]\n",
      " [-0.545  1.388]]\n",
      "Decoder weights\n",
      " [[1.246 0.971]\n",
      " [0.971 2.162]]\n"
     ]
    }
   ],
   "source": [
    "print('Encoder weights\\n', np.round(np.dot(w_encoder, w_encoder.T), 3))\n",
    "print('Decoder weights\\n', np.round(np.dot(w_decoder, w_decoder.T), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoded features uncorrelated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded feature covariance\n",
      " [[ 0.0319451  -0.00556114]\n",
      " [-0.00556114  0.0292503 ]]\n"
     ]
    }
   ],
   "source": [
    "encoder_layer = Model(inputs=autoencoder.inputs, outputs=autoencoder.layers[0].output)\n",
    "encoded_features = np.array(encoder_layer.predict(X_train_scaled))\n",
    "print('Encoded feature covariance\\n', np.cov(encoded_features.T))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weights are not unit norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder weights norm, \n",
      " [1.5014    1.3881999]\n",
      "Decoder weights norm, \n",
      " [1.2459 2.1623]\n"
     ]
    }
   ],
   "source": [
    "print('Encoder weights norm, \\n', np.sum(w_encoder ** 2, axis = 1))\n",
    "print('Decoder weights norm, \\n', np.sum(w_decoder ** 2, axis = 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizing Autoencoder using PCA principles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nb_epoch = 100\n",
    "batch_size = 16\n",
    "input_dim = X_train_scaled.shape[1] #num of predictor variables, \n",
    "encoding_dim = 2\n",
    "learning_rate = 1e-3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Make decoder weights equal to encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Reference: https://stackoverflow.com/questions/53751024/tying-autoencoder-weights-in-a-dense-keras-layer\n",
    "class DenseTied(Layer):\n",
    "    def __init__(self, units,\n",
    "                 activation=None,\n",
    "                 use_bias=True,\n",
    "                 kernel_initializer='glorot_uniform',\n",
    "                 bias_initializer='zeros',\n",
    "                 kernel_regularizer=None,\n",
    "                 bias_regularizer=None,\n",
    "                 activity_regularizer=None,\n",
    "                 kernel_constraint=None,\n",
    "                 bias_constraint=None,\n",
    "                 tied_to=None,\n",
    "                 **kwargs):\n",
    "        self.tied_to = tied_to\n",
    "        if 'input_shape' not in kwargs and 'input_dim' in kwargs:\n",
    "            kwargs['input_shape'] = (kwargs.pop('input_dim'),)\n",
    "        super().__init__(**kwargs)\n",
    "        self.units = units\n",
    "        self.activation = activations.get(activation)\n",
    "        self.use_bias = use_bias\n",
    "        self.kernel_initializer = initializers.get(kernel_initializer)\n",
    "        self.bias_initializer = initializers.get(bias_initializer)\n",
    "        self.kernel_regularizer = regularizers.get(kernel_regularizer)\n",
    "        self.bias_regularizer = regularizers.get(bias_regularizer)\n",
    "        self.activity_regularizer = regularizers.get(activity_regularizer)\n",
    "        self.kernel_constraint = constraints.get(kernel_constraint)\n",
    "        self.bias_constraint = constraints.get(bias_constraint)\n",
    "        self.input_spec = InputSpec(min_ndim=2)\n",
    "        self.supports_masking = True\n",
    "                \n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) >= 2\n",
    "        input_dim = input_shape[-1]\n",
    "\n",
    "        if self.tied_to is not None:\n",
    "            self.kernel = K.transpose(self.tied_to.kernel)\n",
    "            self._non_trainable_weights.append(self.kernel)\n",
    "        else:\n",
    "            self.kernel = self.add_weight(shape=(input_dim, self.units),\n",
    "                                          initializer=self.kernel_initializer,\n",
    "                                          name='kernel',\n",
    "                                          regularizer=self.kernel_regularizer,\n",
    "                                          constraint=self.kernel_constraint)\n",
    "        if self.use_bias:\n",
    "            self.bias = self.add_weight(shape=(self.units,),\n",
    "                                        initializer=self.bias_initializer,\n",
    "                                        name='bias',\n",
    "                                        regularizer=self.bias_regularizer,\n",
    "                                        constraint=self.bias_constraint)\n",
    "        else:\n",
    "            self.bias = None\n",
    "        self.input_spec = InputSpec(min_ndim=2, axes={-1: input_dim})\n",
    "        self.built = True\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        assert input_shape and len(input_shape) >= 2\n",
    "        output_shape = list(input_shape)\n",
    "        output_shape[-1] = self.units\n",
    "        return tuple(output_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        output = K.dot(inputs, self.kernel)\n",
    "        if self.use_bias:\n",
    "            output = K.bias_add(output, self.bias, data_format='channels_last')\n",
    "        if self.activation is not None:\n",
    "            output = self.activation(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_5 (Dense)              (None, 2)                 12        \n",
      "_________________________________________________________________\n",
      "dense_tied_1 (DenseTied)     (None, 5)                 10        \n",
      "=================================================================\n",
      "Total params: 22\n",
      "Trainable params: 12\n",
      "Non-trainable params: 10\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x135515e80>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder = Dense(encoding_dim, activation=\"linear\", input_shape=(input_dim,), use_bias = True) \n",
    "decoder = DenseTied(input_dim, activation=\"linear\", tied_to=encoder, use_bias = False)\n",
    "\n",
    "autoencoder = Sequential()\n",
    "autoencoder.add(encoder)\n",
    "autoencoder.add(decoder)\n",
    "\n",
    "autoencoder.compile(metrics=['accuracy'],\n",
    "                    loss='mean_squared_error',\n",
    "                    optimizer='sgd')\n",
    "autoencoder.summary()\n",
    "\n",
    "autoencoder.fit(X_train_scaled, X_train_scaled,\n",
    "                epochs=nb_epoch,\n",
    "                batch_size=batch_size,\n",
    "                shuffle=True,\n",
    "                verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder weights\n",
      " [[-0.261  0.259 -0.916  0.225 -0.274]\n",
      " [ 0.408  0.676  0.022  0.706  0.356]]\n",
      "Decoder weights\n",
      " [[-0.261  0.259 -0.916  0.225 -0.274]\n",
      " [ 0.408  0.676  0.022  0.706  0.356]]\n"
     ]
    }
   ],
   "source": [
    "w_encoder = np.round(np.transpose(autoencoder.layers[0].get_weights()[0]), 3)\n",
    "w_decoder = np.round(autoencoder.layers[1].get_weights()[0], 3)\n",
    "print('Encoder weights\\n', w_encoder)\n",
    "print('Decoder weights\\n', w_decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train reconstrunction error\n",
      " 0.020553788540722952\n",
      "Test reconstrunction error\n",
      " 0.02011294850382963\n"
     ]
    }
   ],
   "source": [
    "train_predictions = autoencoder.predict(X_train_scaled)\n",
    "print('Train reconstrunction error\\n', sklearn.metrics.mean_squared_error(X_train_scaled, train_predictions))\n",
    "test_predictions = autoencoder.predict(X_test_scaled)\n",
    "print('Test reconstrunction error\\n', sklearn.metrics.mean_squared_error(X_test_scaled, test_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Add weights orthogonality constraint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class WeightsOrthogonalityConstraint (Constraint):\n",
    "    def __init__(self, encoding_dim, weightage = 1):\n",
    "        self.encoding_dim = encoding_dim\n",
    "        self.weightage = weightage\n",
    "        \n",
    "    def fro_norm(m):\n",
    "        return self.weightage * K.sqrt(K.sum(K.square(K.abs(m))))\n",
    "\n",
    "    def weights_orthogonality(self, w):\n",
    "        if(self.encoding_dim > 1):\n",
    "            print(w)\n",
    "            m = K.dot(K.transpose(w), w) - K.eye(self.encoding_dim)\n",
    "            print(m)\n",
    "#             return self.fro_norm(m)\n",
    "            return self.weightage * K.sqrt(K.sum(K.square(K.abs(m))))\n",
    "        else:\n",
    "            m = K.sum(w ** 2) - 1.\n",
    "            return m\n",
    "\n",
    "    def __call__(self, w):\n",
    "        return self.weights_orthogonality(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'dense_6/kernel:0' shape=(5, 2) dtype=float32_ref>\n",
      "Tensor(\"dense_6/weight_regularizer/sub:0\", shape=(2, 2), dtype=float32)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_6 (Dense)              (None, 2)                 12        \n",
      "_________________________________________________________________\n",
      "dense_tied_2 (DenseTied)     (None, 5)                 10        \n",
      "=================================================================\n",
      "Total params: 22\n",
      "Trainable params: 12\n",
      "Non-trainable params: 10\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1098c4390>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder = Dense(encoding_dim, activation=\"linear\", input_shape=(input_dim,), use_bias = True, kernel_regularizer=WeightsOrthogonalityConstraint(encoding_dim)) \n",
    "decoder = DenseTied(input_dim, activation=\"linear\", tied_to=encoder, use_bias = False)\n",
    "\n",
    "autoencoder = Sequential()\n",
    "autoencoder.add(encoder)\n",
    "autoencoder.add(decoder)\n",
    "\n",
    "autoencoder.compile(metrics=['accuracy'],\n",
    "                    loss='mean_squared_error',\n",
    "                    optimizer='sgd')\n",
    "autoencoder.summary()\n",
    "\n",
    "autoencoder.fit(X_train_scaled, X_train_scaled,\n",
    "                epochs=nb_epoch,\n",
    "                batch_size=batch_size,\n",
    "                shuffle=True,\n",
    "                verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.64585453 -0.15490086]\n",
      " [-0.27592906 -0.3519888 ]\n",
      " [ 0.27592278 -0.6416658 ]\n",
      " [-0.6119648  -0.2081838 ]\n",
      " [ 0.25840914 -0.6213636 ]]\n",
      "Encoder weights dot product\n",
      " [[ 1.0106746  -0.01304772]\n",
      " [-0.01304772  0.9890586 ]]\n"
     ]
    }
   ],
   "source": [
    "w_encoder = autoencoder.layers[0].get_weights()[0]\n",
    "print(w_encoder)\n",
    "print('Encoder weights dot product\\n', np.round(np.dot(w_encoder.T, w_encoder), 20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.03803921, -0.00182756],\n",
       "       [-0.00182756,  0.01676502]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.dot(X_train_scaled, w_encoder)\n",
    "np.cov(a.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train reconstrunction error\n",
      " 0.01767556744993414\n",
      "Test reconstrunction error\n",
      " 0.016522204126475946\n"
     ]
    }
   ],
   "source": [
    "train_predictions = autoencoder.predict(X_train_scaled)\n",
    "print('Train reconstrunction error\\n', sklearn.metrics.mean_squared_error(X_train_scaled, train_predictions))\n",
    "test_predictions = autoencoder.predict(X_test_scaled)\n",
    "print('Test reconstrunction error\\n', sklearn.metrics.mean_squared_error(X_test_scaled, test_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Encoded features are uncorrelated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.regularizers import l1, Regularizer\n",
    "np.linalg.eigvals\n",
    "class UncorrelatedFeaturesConstraint (Constraint):\n",
    "    def __init__(self, encoding_dim, correlation_weight = 1.0, eigen_decay_weight = 0.0):\n",
    "        self.encoding_dim = encoding_dim\n",
    "        self.correlation_weight = correlation_weight\n",
    "        self.eigen_decay_weight = eigen_decay_weight\n",
    "        \n",
    "    def fro_norm(m):\n",
    "        return self.weightage * K.sqrt(K.sum(K.square(K.abs(m))))\n",
    "    \n",
    "    def get_covariance(self, x):\n",
    "        x_centered_list = []\n",
    "        for i in range(self.encoding_dim):\n",
    "            x_centered_list.append(x[:, i] - K.mean(x[:, i]))\n",
    "        x_centered = tf.stack(x_centered_list)\n",
    "        covariance = K.dot(x_centered, K.transpose(x_centered)) / tf.cast(x_centered.get_shape()[0], tf.float32)\n",
    "        return covariance\n",
    "            \n",
    "        \n",
    "    # Constraint penalties\n",
    "    def uncorrelated_feature(self, x):\n",
    "        if(self.encoding_dim <= 1):\n",
    "            return 0.0\n",
    "        else:\n",
    "            output = K.sum(K.square(self.covariance - K.dot(self.covariance, K.eye(self.encoding_dim)))) ** 0.5\n",
    "            return output\n",
    "\n",
    "    def eigenvalue_decay(self, x):\n",
    "        power = 10  # number of iterations of the power method\n",
    "#         o = tf.cast(np.ones(shape=(self.encoding_dim,1)), tf.float32)  # initial values for the dominant eigenvector\n",
    "        o = K.ones(shape=(self.encoding_dim,1))\n",
    "        o = K.constant(np.ones(shape=(self.encoding_dim,1)))\n",
    "        print('oo', o)\n",
    "        print('cov', self.covariance)\n",
    "        # power method for approximating the dominant eigenvector:\n",
    "#         domin_eigenvect = K.dot(self.covariance, K.transpose(K.ones(shape=(self.encoding_dim,1))))\n",
    "        domin_eigenvect = K.sum(self.covariance, axis = 1)\n",
    "        for n in range(power):\n",
    "            1\n",
    "#             domin_eigenvect = K.dot(self.covariance, domin_eigenvect)    \n",
    "        \n",
    "        WWd = K.dot(self.covariance, domin_eigenvect)\n",
    "#         domin_eigenval = K.dot(WWd, domin_eigenvect) / K.dot(domin_eigenvect, domin_eigenvect)  # the corresponding dominant eigenvalue\n",
    "        domin_eigenval = 1\n",
    "        return domin_eigenval ** 0.5\n",
    "\n",
    "    def __call__(self, x):\n",
    "        self.covariance = self.get_covariance(x)\n",
    "        self.eigenvalue_decay(x)\n",
    "        return self.correlation_weight * self.uncorrelated_feature(x) #+ self.eigen_decay_weight * self.eigenvalue_decay(x)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "oo Tensor(\"dense_92/activity_regularizer/Const_3:0\", shape=(3, 1), dtype=float32)\n",
      "cov Tensor(\"dense_92/activity_regularizer/truediv:0\", shape=(3, 3), dtype=float32)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Shape must be rank 2 but is rank 1 for 'dense_92/activity_regularizer/MatMul_1' (op: 'MatMul') with input shapes: [3,3], [3].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m~/virtualenvs/keras_tf/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_create_c_op\u001b[0;34m(graph, node_def, inputs, control_inputs)\u001b[0m\n\u001b[1;32m   1658\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1659\u001b[0;31m     \u001b[0mc_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_FinishOperation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop_desc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1660\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Shape must be rank 2 but is rank 1 for 'dense_92/activity_regularizer/MatMul_1' (op: 'MatMul') with input shapes: [3,3], [3].",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-245-0382f4080ceb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mautoencoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mautoencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mautoencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/virtualenvs/keras_tf/lib/python3.6/site-packages/keras/engine/sequential.py\u001b[0m in \u001b[0;36madd\u001b[0;34m(self, layer)\u001b[0m\n\u001b[1;32m    163\u001b[0m                     \u001b[0;31m# and create the node connecting the current layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m                     \u001b[0;31m# to the input layer we just created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m                     \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m                     \u001b[0mset_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/virtualenvs/keras_tf/lib/python3.6/site-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[1;32m    503\u001b[0m                     regularization_losses = [\n\u001b[1;32m    504\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivity_regularizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 505\u001b[0;31m                         for x in to_list(output)]\n\u001b[0m\u001b[1;32m    506\u001b[0m                 self.add_loss(regularization_losses,\n\u001b[1;32m    507\u001b[0m                               inputs=to_list(inputs))\n",
      "\u001b[0;32m~/virtualenvs/keras_tf/lib/python3.6/site-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    503\u001b[0m                     regularization_losses = [\n\u001b[1;32m    504\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivity_regularizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 505\u001b[0;31m                         for x in to_list(output)]\n\u001b[0m\u001b[1;32m    506\u001b[0m                 self.add_loss(regularization_losses,\n\u001b[1;32m    507\u001b[0m                               inputs=to_list(inputs))\n",
      "\u001b[0;32m<ipython-input-244-d8557804cd11>\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcovariance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_covariance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meigenvalue_decay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorrelation_weight\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muncorrelated_feature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#+ self.eigen_decay_weight * self.eigenvalue_decay(x)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-244-d8557804cd11>\u001b[0m in \u001b[0;36meigenvalue_decay\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;31m#             domin_eigenvect = K.dot(self.covariance, domin_eigenvect)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m         \u001b[0mWWd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcovariance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdomin_eigenvect\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;31m#         domin_eigenval = K.dot(WWd, domin_eigenvect) / K.dot(domin_eigenvect, domin_eigenvect)  # the corresponding dominant eigenvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mdomin_eigenval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/virtualenvs/keras_tf/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36mdot\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m   1083\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse_tensor_dense_matmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1084\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1085\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1086\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1087\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/virtualenvs/keras_tf/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py\u001b[0m in \u001b[0;36mmatmul\u001b[0;34m(a, b, transpose_a, transpose_b, adjoint_a, adjoint_b, a_is_sparse, b_is_sparse, name)\u001b[0m\n\u001b[1;32m   2453\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2454\u001b[0m       return gen_math_ops.mat_mul(\n\u001b[0;32m-> 2455\u001b[0;31m           a, b, transpose_a=transpose_a, transpose_b=transpose_b, name=name)\n\u001b[0m\u001b[1;32m   2456\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2457\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/virtualenvs/keras_tf/lib/python3.6/site-packages/tensorflow/python/ops/gen_math_ops.py\u001b[0m in \u001b[0;36mmat_mul\u001b[0;34m(a, b, transpose_a, transpose_b, name)\u001b[0m\n\u001b[1;32m   5331\u001b[0m   _, _, _op = _op_def_lib._apply_op_helper(\n\u001b[1;32m   5332\u001b[0m         \u001b[0;34m\"MatMul\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtranspose_a\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtranspose_a\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtranspose_b\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtranspose_b\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5333\u001b[0;31m                   name=name)\n\u001b[0m\u001b[1;32m   5334\u001b[0m   \u001b[0m_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5335\u001b[0m   \u001b[0m_inputs_flat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/virtualenvs/keras_tf/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    786\u001b[0m         op = g.create_op(op_type_name, inputs, output_types, name=scope,\n\u001b[1;32m    787\u001b[0m                          \u001b[0minput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattr_protos\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 788\u001b[0;31m                          op_def=op_def)\n\u001b[0m\u001b[1;32m    789\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0moutput_structure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_stateful\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    790\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/virtualenvs/keras_tf/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    505\u001b[0m                 \u001b[0;34m'in a future version'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'after %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m                 instructions)\n\u001b[0;32m--> 507\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    508\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    509\u001b[0m     doc = _add_deprecated_arg_notice_to_docstring(\n",
      "\u001b[0;32m~/virtualenvs/keras_tf/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mcreate_op\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   3298\u001b[0m           \u001b[0minput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3299\u001b[0m           \u001b[0moriginal_op\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_default_original_op\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3300\u001b[0;31m           op_def=op_def)\n\u001b[0m\u001b[1;32m   3301\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_op_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompute_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompute_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3302\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/virtualenvs/keras_tf/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, node_def, g, inputs, output_types, control_inputs, input_types, original_op, op_def)\u001b[0m\n\u001b[1;32m   1821\u001b[0m           op_def, inputs, node_def.attr)\n\u001b[1;32m   1822\u001b[0m       self._c_op = _create_c_op(self._graph, node_def, grouped_inputs,\n\u001b[0;32m-> 1823\u001b[0;31m                                 control_input_ops)\n\u001b[0m\u001b[1;32m   1824\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1825\u001b[0m     \u001b[0;31m# Initialize self._outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/virtualenvs/keras_tf/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_create_c_op\u001b[0;34m(graph, node_def, inputs, control_inputs)\u001b[0m\n\u001b[1;32m   1660\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1661\u001b[0m     \u001b[0;31m# Convert to ValueError for backwards compatibility.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1662\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1663\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1664\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mc_op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Shape must be rank 2 but is rank 1 for 'dense_92/activity_regularizer/MatMul_1' (op: 'MatMul') with input shapes: [3,3], [3]."
     ]
    }
   ],
   "source": [
    "encoding_dim = 3\n",
    "encoder = Dense(encoding_dim, activation=\"linear\", input_shape=(input_dim,), use_bias = True, activity_regularizer=UncorrelatedFeaturesConstraint(encoding_dim, correlation_weight = 1.)) \n",
    "# encoder = Dense(encoding_dim, activation=\"linear\", input_shape=(input_dim,), use_bias = True, activity_regularizer=l1(0.1)) \n",
    "# encoder = Dense(encoding_dim, activation=\"linear\", input_shape=(input_dim,), use_bias = True) \n",
    "decoder = DenseTied(input_dim, activation=\"linear\", tied_to=encoder, use_bias = False)\n",
    "\n",
    "autoencoder = Sequential()\n",
    "autoencoder.add(encoder)\n",
    "autoencoder.add(decoder)\n",
    "\n",
    "autoencoder.compile(metrics=['accuracy'],\n",
    "                    loss='mean_squared_error',\n",
    "                    optimizer='adam')\n",
    "autoencoder.summary()\n",
    "\n",
    "autoencoder.fit(X_train_scaled, X_train_scaled,\n",
    "                epochs=nb_epoch,\n",
    "                batch_size=batch_size,\n",
    "                shuffle=True,\n",
    "                verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class EigenvalueRegularizer(Regularizer):\n",
    "    \"\"\"This class implements the Eigenvalue Decay regularizer.\n",
    "    \n",
    "    Args:\n",
    "        The constant that controls the regularization on the current layer\n",
    "        ( see Section 3 of https://arxiv.org/abs/1604.06985 )\n",
    "    Returns:\n",
    "        The regularized loss (for the training data) and\n",
    "        the original loss (for the validation data).\n",
    "        \n",
    "    \"\"\"\n",
    "    def __init__(self, k):\n",
    "        self.k = k\n",
    "        self.uses_learning_phase = True\n",
    "\n",
    "\n",
    "    def set_param(self, p):\n",
    "        self.p = p\n",
    "\n",
    "\n",
    "    def __call__(self, loss):\n",
    "        power = 9  # number of iterations of the power method\n",
    "        W = self.p\n",
    "        WW = K.dot(K.transpose(W), W)\n",
    "        dim1, dim2 = K.eval(K.shape(WW))\n",
    "        k = self.k\n",
    "        o = np.ones(dim1)  # initial values for the dominant eigenvector\n",
    "\n",
    "        # power method for approximating the dominant eigenvector:\n",
    "        o = np.array([1., 1., 1.])\n",
    "        domin_eigenvect = K.dot(WW, o)\n",
    "        for n in range(power - 1):\n",
    "            domin_eigenvect = K.dot(WW, domin_eigenvect)    \n",
    "        \n",
    "        WWd = K.dot(WW, domin_eigenvect)\n",
    "        domin_eigenval = K.dot(WWd, domin_eigenvect) / K.dot(domin_eigenvect, domin_eigenvect)  # the corresponding dominant eigenvalue\n",
    "        regularized_loss = loss + (domin_eigenval ** 0.5) * self.k  # multiplied by the given regularization gain\n",
    "        return K.in_train_phase(regularized_loss, loss)\n",
    "    \n",
    "\n",
    "    def get_config(self):\n",
    "        return {\"name\": self.__class__.__name__,\n",
    "                \"k\": self.k}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder weights dot product\n",
      " [[ 1.0126897  -0.01281005  0.04509516]\n",
      " [-0.01281005  1.000313    0.00253354]\n",
      " [ 0.04509516  0.00253354  0.96785223]]\n"
     ]
    }
   ],
   "source": [
    "w_encoder = autoencoder.layers[0].get_weights()[0]\n",
    "print('Encoder weights dot product\\n', np.round(np.dot(w_encoder.T, w_encoder), 20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500, 3)\n",
      "Encoded feature covariance\n",
      " [[0.01719019 0.01223717 0.00116263]\n",
      " [0.01223717 0.06834599 0.01161249]\n",
      " [0.00116263 0.01161249 0.01674539]]\n"
     ]
    }
   ],
   "source": [
    "encoder_layer = Model(inputs=autoencoder.inputs, outputs=autoencoder.layers[0].output)\n",
    "encoded_features = np.array(encoder_layer.predict(X_train_scaled))\n",
    "# encoded_features\n",
    "print(encoded_features.shape)\n",
    "print('Encoded feature covariance\\n', np.cov(encoded_features.T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx = encoded_features\n",
    "xx_diff = xx\n",
    "mu = np.mean(xx, axis=0)\n",
    "for i in range(xx.shape[1]):\n",
    "    for j in range(xx.shape[1]):\n",
    "        a = xx[:, i] - np.mean(xx[:, i])\n",
    "        b = xx[:, j] - np.mean(xx[:, j])\n",
    "        print(np.sum(a * b) / (xx.shape[0] - 1))\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.cov(xx.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aa = np.array(np.sum(xx_diff, axis = 0))\n",
    "(aa.T + np.zeros((2,1)))[:,0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.dot(encoded_features.T, encoded_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.dot(X_train_scaled, w_encoder)\n",
    "np.cov(a.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_predictions = autoencoder.predict(X_train_scaled)\n",
    "print('Train reconstrunction error\\n', sklearn.metrics.mean_squared_error(X_train_scaled, train_predictions))\n",
    "test_predictions = autoencoder.predict(X_test_scaled)\n",
    "print('Test reconstrunction error\\n', sklearn.metrics.mean_squared_error(X_test_scaled, test_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Unit norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Dense(encoding_dim, activation=\"linear\", input_shape=(input_dim,), use_bias = True, kernel_regularizer=OrthogonalityConstraint(encoding_dim), kernel_constraint=UnitNorm(axis=0)) \n",
    "decoder = DenseTied(input_dim, activation=\"linear\", tied_to=encoder, use_bias = False)\n",
    "\n",
    "autoencoder = Sequential()\n",
    "autoencoder.add(encoder)\n",
    "autoencoder.add(decoder)\n",
    "\n",
    "autoencoder.compile(metrics=['accuracy'],\n",
    "                    loss='mean_squared_error',\n",
    "                    optimizer='sgd')\n",
    "autoencoder.summary()\n",
    "\n",
    "autoencoder.fit(X_train_scaled, X_train_scaled,\n",
    "                epochs=nb_epoch,\n",
    "                batch_size=batch_size,\n",
    "                shuffle=True,\n",
    "                verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_encoder = autoencoder.layers[0].get_weights()[0]\n",
    "print('Encoder weights dot product\\n', np.round(np.dot(w_encoder.T, w_encoder), 20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_predictions = autoencoder.predict(X_train_scaled)\n",
    "print('Train reconstrunction error\\n', sklearn.metrics.mean_squared_error(X_train_scaled, train_predictions))\n",
    "test_predictions = autoencoder.predict(X_test_scaled)\n",
    "print('Test reconstrunction error\\n', sklearn.metrics.mean_squared_error(X_test_scaled, test_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All of them together with a nonlinear Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_dim = 100\n",
    "cov = sklearn.datasets.make_spd_matrix(n_dim, random_state=None)\n",
    "mu = np.random.normal(0, 0.1, n_dim)\n",
    "n = 5000\n",
    "\n",
    "X_large = np.random.multivariate_normal(mu, cov, n)\n",
    "X_large_train, X_large_test = train_test_split(X_large, test_size=0.5, random_state=123)\n",
    "\n",
    "# scaler_large = StandardScaler().fit(X_large_train)\n",
    "scaler_large = MinMaxScaler().fit(X_large_train)\n",
    "\n",
    "X_large_train_scaled = scaler_large.fit_transform(X_large_train)\n",
    "\n",
    "X_large_test_scaled = scaler_large.fit_transform(X_large_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_epoch = 100\n",
    "batch_size = 16\n",
    "input_dim = X_large_train_scaled.shape[1] #num of predictor variables, \n",
    "encoding_dim = 8\n",
    "\n",
    "# Plain vanilla\n",
    "encoder1 = Dense(encoding_dim, activation=\"relu\", input_shape=(input_dim,), use_bias = True) \n",
    "encoder2 = Dense(4, activation=\"relu\", input_shape=(encoding_dim,), use_bias = True) \n",
    "decoder2 = Dense(encoding_dim, activation=\"relu\", use_bias = True)\n",
    "decoder1 = Dense(input_dim, activation=\"relu\", use_bias = True)\n",
    "\n",
    "# 1. Weight orthogonal\n",
    "encoder1 = Dense(encoding_dim, activation=\"relu\", input_shape=(input_dim,), use_bias = True, kernel_regularizer=OrthogonalityConstraint(encoding_dim), kernel_constraint=UnitNorm(axis=0)) \n",
    "encoder2 = Dense(4, activation=\"relu\", input_shape=(encoding_dim,), use_bias = True, kernel_regularizer=OrthogonalityConstraint(4), kernel_constraint=UnitNorm(axis=0)) \n",
    "decoder2 = DenseTied(encoding_dim, activation=\"relu\", tied_to=encoder2, use_bias = False)\n",
    "decoder1 = DenseTied(input_dim, activation=\"relu\", tied_to=encoder1, use_bias = False)\n",
    "\n",
    "# encoder1 = Dense(encoding_dim, activation=\"relu\", input_shape=(input_dim,), use_bias = True, kernel_regularizer=OrthogonalityConstraint(encoding_dim), kernel_constraint=UnitNorm(axis=0)) \n",
    "# encoder2 = Dense(4, activation=\"relu\", input_shape=(encoding_dim,), use_bias = True, kernel_regularizer=OrthogonalityConstraint(4), kernel_constraint=UnitNorm(axis=0)) \n",
    "# decoder2 = DenseTied(encoding_dim, activation=\"relu\", tied_to=encoder2, use_bias = False)\n",
    "# decoder1 = DenseTied(input_dim, activation=\"relu\", tied_to=encoder1, use_bias = False)\n",
    "\n",
    "\n",
    "# encoder1 = Dense(encoding_dim, activation=\"relu\", input_shape=(input_dim,), use_bias = True, kernel_regularizer=OrthogonalityConstraint(encoding_dim), kernel_constraint=UnitNorm(axis=0)) \n",
    "# encoder2 = Dense(4, activation=\"relu\", input_shape=(encoding_dim,), use_bias = True, kernel_regularizer=OrthogonalityConstraint(4), kernel_constraint=UnitNorm(axis=0)) \n",
    "# decoder2 = DenseTied(encoding_dim, activation=\"relu\", tied_to=encoder2, use_bias = False)\n",
    "# decoder1 = DenseTied(input_dim, activation=\"relu\", tied_to=encoder1, use_bias = False)\n",
    "\n",
    "# encoder1 = Dense(encoding_dim, activation=\"relu\", input_shape=(input_dim,), use_bias = True, kernel_regularizer=OrthogonalityConstraint(encoding_dim), kernel_constraint=UnitNorm(axis=0)) \n",
    "# encoder2 = Dense(4, activation=\"relu\", input_shape=(encoding_dim,), use_bias = True, kernel_regularizer=OrthogonalityConstraint(4), kernel_constraint=UnitNorm(axis=0)) \n",
    "# decoder2 = Dense(encoding_dim, activation=\"relu\", use_bias = True)\n",
    "# decoder1 = Dense(input_dim, activation=\"relu\", use_bias = True)\n",
    "\n",
    "\n",
    "# encoder1 = Dense(encoding_dim, activation=\"relu\", input_shape=(input_dim,), use_bias = True) \n",
    "# encoder2 = Dense(4, activation=\"relu\", input_shape=(encoding_dim,), use_bias = True) \n",
    "# decoder2 = DenseTied(encoding_dim, activation=\"relu\", use_bias = True)\n",
    "# decoder1 = DenseTied(input_dim, activation=\"relu\", use_bias = True)\n",
    "\n",
    "\n",
    "autoencoder = Sequential()\n",
    "autoencoder.add(encoder1)\n",
    "autoencoder.add(encoder2)\n",
    "autoencoder.add(decoder2)\n",
    "autoencoder.add(decoder1)\n",
    "\n",
    "autoencoder.compile(metrics=['accuracy'],\n",
    "                    loss='mean_squared_error',\n",
    "                    optimizer='adam')\n",
    "autoencoder.summary()\n",
    "\n",
    "autoencoder.fit(X_large_train_scaled, X_large_train_scaled,\n",
    "                epochs=nb_epoch,\n",
    "                batch_size=batch_size,\n",
    "                shuffle=True,\n",
    "                verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_predictions = autoencoder.predict(X_large_train_scaled)\n",
    "print('Train reconstrunction error\\n', sklearn.metrics.mean_squared_error(X_large_train_scaled, train_predictions))\n",
    "test_predictions = autoencoder.predict(X_large_test_scaled)\n",
    "print('Test reconstrunction error\\n', sklearn.metrics.mean_squared_error(X_large_test_scaled, test_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_encoder = autoencoder.layers[0].get_weights()[0]\n",
    "print(w_encoder)\n",
    "print('Encoder weights dot product\\n', np.round(np.dot(w_encoder.T, w_encoder), 20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Encoded features are uncorrelated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_layer = Model(inputs=autoencoder.inputs, outputs=autoencoder.layers[0].output)\n",
    "encoded_features = np.array(encoder_layer.predict(X_train_scaled))\n",
    "# encoded_features\n",
    "print(encoded_features.shape)\n",
    "print('Encoded feature covariance\\n', np.cov(encoded_features.T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoded1 = Dense(4, activation=\"sigmoid\", input_shape=(4,), use_bias=True)\n",
    "# decoded1 = DenseTied(4, activation=\"sigmoid\", tied_to=encoded1, use_bias=False)\n",
    "\n",
    "# # autoencoder\n",
    "# #\n",
    "# autoencoder = Sequential()\n",
    "# # autoencoder.add(input_)\n",
    "# autoencoder.add(encoded1)\n",
    "# autoencoder.add(decoded1)\n",
    "\n",
    "# autoencoder.compile(optimizer=\"adam\", loss=\"binary_crossentropy\")\n",
    "\n",
    "# print(autoencoder.summary())\n",
    "\n",
    "# autoencoder.fit(x=np.random.rand(100, 4), y=np.random.randint(0, 1, size=(100, 4)))\n",
    "\n",
    "# print(autoencoder.layers[0].get_weights()[0])\n",
    "# print(autoencoder.layers[1].get_weights()[0])\n",
    "\n",
    "# input_ = Input(shape=(16,), dtype=np.float32)\n",
    "# encoder\n",
    "#\n",
    "\n",
    "def fro_norm(w):\n",
    "    return K.sqrt(K.sum(K.square(K.abs(w))))\n",
    "\n",
    "def cust_reg(w):\n",
    "    if(encoding_dim > 1):\n",
    "        m = K.dot(K.transpose(w), w) - K.eye(encoding_dim)\n",
    "        return fro_norm(m)\n",
    "    else:\n",
    "        m = K.sum(w ** 2) - 1.\n",
    "        return m\n",
    "    \n",
    "nb_epoch = 100\n",
    "batch_size = 16\n",
    "input_dim = X_scaled.shape[1] #num of predictor variables, \n",
    "encoding_dim = 1\n",
    "learning_rate = 1e-3\n",
    "encoder = Dense(encoding_dim, activation=\"linear\", input_shape=(input_dim,), use_bias = True, kernel_regularizer=cust_reg, kernel_constraint=UnitNorm(axis=0)) \n",
    "decoder = DenseTied(input_dim, activation=\"linear\", tied_to=encoder, use_bias = False)\n",
    "\n",
    "autoencoder = Sequential()\n",
    "autoencoder.add(encoder)\n",
    "autoencoder.add(decoder)\n",
    "\n",
    "autoencoder.compile(metrics=['accuracy'],\n",
    "                    loss='mean_squared_error',\n",
    "                    optimizer='sgd')\n",
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.fit(X_scaled, X_scaled,\n",
    "                    epochs=nb_epoch,\n",
    "                    batch_size=batch_size,\n",
    "                    shuffle=True,\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.round(np.transpose(autoencoder.layers[0].get_weights()[0]), 3))\n",
    "print(np.round(autoencoder.layers[1].get_weights()[0], 3))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scipy.linalg.norm(np.transpose(autoencoder.layers[0].get_weights()[0]), 2)\n",
    "np.sum(np.transpose(autoencoder.layers[0].get_weights()[0]) ** 2, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.round(pca.components_, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.round(np.dot(pca.components_, np.transpose(pca.components_)), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.round(np.dot(autoencoder.layers[0].get_weights()[0], np.transpose(autoencoder.layers[0].get_weights()[0])), 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "-------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras import backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "\n",
    "a_dim = 16\n",
    "from keras import backend as K\n",
    "def fro_norm(w):\n",
    "    return K.sqrt(K.sum(K.square(K.abs(w))))\n",
    "\n",
    "def cust_reg(w):\n",
    "    print(w.shape[1])\n",
    "    m = K.dot(K.transpose(w), w) - np.eye(a_dim)\n",
    "    return fro_norm(m)\n",
    "\n",
    "X = np.random.randn(100, 100)\n",
    "y = np.random.randint(2, size=(100, 1))\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "\n",
    "# apply regularization here. applies regularization to the \n",
    "# output (activation) of the layer\n",
    "model.add(Dense(a_dim, input_shape=(100,), \n",
    "                kernel_regularizer=cust_reg))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss=\"binary_crossentropy\",\n",
    "              optimizer='sgd',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(X, y, epochs=100, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "intermediate_layer = Model(inputs=model.inputs, outputs=model.layers[0].output)\n",
    "intermediate_output = intermediate_layer.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intermediate_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.round(np.dot(np.transpose(model.layers[0].get_weights()[0]), model.layers[0].get_weights()[0]), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layers[0].get_weights()[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solve PCA by reconstruction loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scipy\n",
    "from scipy.optimize import minimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scipy.linalg.norm(X_scaled, ord=2, axis=None, keepdims=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.diag(np.cov(X_scaled.T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.eig(np.cov(X_scaled.T))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Vp = np.linalg.eig(np.cov(X_scaled.T))[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = np.dot(X_scaled, Vp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.round(np.cov(Z.T), 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.dot(Vp, np.dot(np.dot(X_scaled.T, X_scaled), Vp.T)) /"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reconstruction_error(x):\n",
    "    V = x.reshape(-1, 5)\n",
    "    loss = 0.1 * scipy.linalg.norm((X_train_scaled - np.dot(X_train_scaled, np.dot(np.transpose(V), V))), 2) / V.shape[0] + orthogonality_constraint(V) + norm_constraint(V) + max_variance(V)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstruction_error(pca.components_.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def orthogonality_constraint(V):\n",
    "    return scipy.linalg.norm(np.dot(V, np.transpose(V)) - np.eye(V.shape[0]), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def norm_constraint(V):\n",
    "    return scipy.linalg.norm(np.sum(V ** 2, axis = 1) - np.ones(V.shape[0]), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def max_variance(V):\n",
    "    eigenvalues = np.linalg.eig(np.cov(X_train_scaled.T))[0][0:V.shape[0]]\n",
    "#     print(eigenvalues)\n",
    "    Z_scores = np.dot(X_train_scaled, V.T)\n",
    "    if(V.shape[0] > 1):\n",
    "        Z_cov = np.diag(np.cov(Z_scores.T))\n",
    "    else:\n",
    "        Z_cov = np.cov(Z_scores.T)\n",
    "#     print(Z_cov)\n",
    "    return scipy.linalg.norm(Z_cov - eigenvalues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orthogonality_constraint(pca.components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pca_reduced = decomposition.PCA(n_components=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_reduced.fit(X_train_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_reduced.explained_variance_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_reduced.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstruction_error(pca_reduced.components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_reduced.components_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scipy.linalg.norm(X_train_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orthogonality_constraint(pca_reduced.components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# minimize(reconstruction_error, x0 = pca_reduced.components_.flatten())\n",
    "# result = minimize(reconstruction_error, x0 = np.random.normal(0, 1, len(pca_reduced.components_.flatten())))\n",
    "# result = minimize(reconstruction_error, x0 = np.random.normal(0, 1, 5))\n",
    "# result = minimize(reconstruction_error, x0 = np.zeros(5), method='Nelder-Mead')\n",
    "result = minimize(reconstruction_error, x0 = np.random.normal(0, 1, 10), method='Nelder-Mead')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orthogonality_constraint(result.x.reshape(-1, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(result.x.reshape(-1, 5) ** 2, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.x.reshape(-1, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_variance(result.x.reshape(-1, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = pca_reduced.components_.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_reduced.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "V1 = x.reshape(-1, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(V1 ** 2, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "V2 = result.x.reshape(-1, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(V2 ** 2, axis = 1) - np.ones(V2.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scipy.linalg.norm(np.sum(V2 ** 2, axis = 1) - np.ones(V2.shape[0]), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sqrt(0.01719766 ** 2 + (0.01719765 ** 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = np.array([1, 2, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scipy.linalg.norm(a, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sqrt((1 ** 2 + 2 ** 2 + 3 ** 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0.01719766 ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(-0.01719765 ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.linalg.eig(cc)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "print(keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.eig(np.cov(X_train_scaled.T))[0][0:V.shape[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "keras_tf",
   "language": "python",
   "name": "keras_tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
